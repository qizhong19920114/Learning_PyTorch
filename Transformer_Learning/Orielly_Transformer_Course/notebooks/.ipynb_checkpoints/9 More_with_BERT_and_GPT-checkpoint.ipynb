{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265db32f",
   "metadata": {},
   "source": [
    "## 9.1 Siamese BERT-networks for semantic searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d93f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "\n",
    "from random import sample, seed, shuffle\n",
    "from sentence_transformers import InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89f2aca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.10961687564849854,\n",
       " 'start': 545,\n",
       " 'end': 591,\n",
       " 'answer': 'data scientist, start-up founder, and educator'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERSON = 'Sinan Ozdemir'\n",
    "\n",
    "# Note this is NOT an efficient way to search on google. This is done simply for education purposes\n",
    "google_html = BeautifulSoup(requests.get(f'https://www.google.com/search?q={PERSON}').text).get_text()[:1024]\n",
    "\n",
    "nlp = pipeline('question-answering', \n",
    "               model='deepset/roberta-base-squad2', \n",
    "               tokenizer='deepset/roberta-base-squad2', \n",
    "               max_length=10)\n",
    "\n",
    "nlp(f'Who is {PERSON}?', google_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04e779f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 104 documents/paragraphs\n"
     ]
    }
   ],
   "source": [
    "# textbook about insects\n",
    "text = urlopen('https://www.gutenberg.org/cache/epub/10834/pg10834.txt').read().decode()\n",
    "\n",
    "# strip out header and footer\n",
    "# text = text[text.index(\"*** START OF\") :text.index(\"*** END OF\")]\n",
    "\n",
    "# Only keep documents of at least 50 characters\n",
    "documents = list(filter(lambda x: len(x) > 50, text.split('\\r\\n\\r\\n')))\n",
    "\n",
    "documents = np.array(documents)\n",
    "\n",
    "print(f'There are {len(documents)} documents/paragraphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28bc736d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a model pre-trained on an asymmetric semantic search task\n",
    "sbert_model = SentenceTransformer('msmarco-distilbert-base-v4')\n",
    "\n",
    "sbert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15bc16c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(116, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Documents are encoded by calling model.encode()\n",
    "document_embeddings = sbert_model.encode(documents)\n",
    "\n",
    "document_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83525f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa13735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[-1.1211e-01, -1.0464e-01, -1.0464e-01, -1.0464e-01, -1.0464e-01,\n",
       "         -1.0464e-01, -1.0030e-01, -1.0030e-01, -9.3436e-02, -9.2324e-02,\n",
       "         -9.1212e-02, -8.8146e-02, -6.5523e-02, -6.4774e-02, -6.1653e-02,\n",
       "         -6.0359e-02, -5.9610e-02, -5.9591e-02, -5.6180e-02, -5.5162e-02,\n",
       "         -5.2260e-02, -4.7107e-02, -4.6308e-02, -4.5581e-02, -4.5156e-02,\n",
       "         -4.4223e-02, -4.0409e-02, -3.9424e-02, -3.6698e-02, -3.4323e-02,\n",
       "         -3.4124e-02, -3.3633e-02, -3.1443e-02, -2.7684e-02, -2.7428e-02,\n",
       "         -2.3387e-02, -2.1927e-02, -2.1242e-02, -2.1079e-02, -1.8942e-02,\n",
       "         -1.5934e-02, -1.5326e-02, -1.4372e-02, -1.4291e-02, -1.3951e-02,\n",
       "         -1.3196e-02, -1.2449e-02, -1.1834e-02, -9.5105e-03, -9.4630e-03,\n",
       "         -9.0947e-03, -9.0319e-03, -8.5620e-03, -7.7932e-03, -6.8224e-03,\n",
       "         -6.7419e-03, -2.9636e-03,  3.8625e-04,  1.6381e-03,  4.9331e-03,\n",
       "          5.9962e-03,  6.2124e-03,  6.3127e-03,  7.9495e-03,  8.8445e-03,\n",
       "          1.0241e-02,  1.0334e-02,  1.1236e-02,  1.1236e-02,  1.3378e-02,\n",
       "          1.4233e-02,  1.4400e-02,  1.4860e-02,  1.8440e-02,  1.8993e-02,\n",
       "          1.9858e-02,  2.0844e-02,  2.0844e-02,  2.2611e-02,  2.3841e-02,\n",
       "          2.4335e-02,  2.5309e-02,  2.5817e-02,  3.0545e-02,  3.1845e-02,\n",
       "          3.3467e-02,  3.4185e-02,  3.9502e-02,  3.9992e-02,  4.1659e-02,\n",
       "          4.2773e-02,  4.6813e-02,  4.7229e-02,  4.7403e-02,  4.8913e-02,\n",
       "          5.1736e-02,  5.6567e-02,  5.9238e-02,  5.9833e-02,  6.1292e-02,\n",
       "          6.3881e-02,  6.6463e-02,  6.7226e-02,  6.7524e-02,  8.7041e-02,\n",
       "          9.6907e-02,  1.0881e-01,  1.0992e-01,  1.1140e-01,  1.1825e-01,\n",
       "          1.2716e-01,  1.4172e-01,  1.6190e-01,  1.8479e-01,  2.4794e-01,\n",
       "          4.8995e-01]]),\n",
       "indices=tensor([[ 98,  19,  10,  14,  16,  12,   1,  69,  17, 100,  57, 103,  99,  48,\n",
       "          56,  88,  51, 107,  95,  50,  58,  52,  90,  47,   6,  78,  59,  93,\n",
       "         106,  85,  75,   7,  74,  87,  26,  66, 113, 114,  64,  91,  72,  80,\n",
       "         105,  62, 102,  76, 101,  18,  96,  25,  82,  89,  70,  41,  60,  94,\n",
       "          23,  61,  73,  65,  83,  86,  68,  22,  77,   5, 115,  13,  15,   8,\n",
       "         104, 112,  53,  97,  92,  31,  11,   9,  81,  84,  29,  63,  67,  39,\n",
       "          55,  40,   3,   4,  43,  71, 109,  49, 110,  42,  27, 111,  34,  24,\n",
       "           0,  20, 108,  21,  54,  46,  79,  44,  33,  28,   2,  45,  38,  36,\n",
       "          32,  37,  35,  30]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUESTION = 'How many horns does a flea have?'  # a natural language query\n",
    "\n",
    "query_embedding = sbert_model.encode(QUESTION)  # embed the query into a vector space\n",
    "\n",
    "top_scores = util.cos_sim(query_embedding, document_embeddings)  # use cosine similarity to find the most relevant document\n",
    "\n",
    "top_scores.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada5d037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Document 1 Cos_Sim 0.490:\n",
      "\n",
      "When examined by a microscope, the flea is a pleasant object. The body\r\n",
      "is curiously adorned with a suit of polished armour, neatly jointed, and\r\n",
      "beset with a great number of sharp pins almost like the quills of a\r\n",
      "porcupine: it has a small head, large eyes, two horns, or feelers, which\r\n",
      "proceed from the head, and four long legs from the breast; they are very\r\n",
      "hairy and long, and have several joints, which fold as it were one\r\n",
      "within another.\n",
      "\n",
      "\n",
      "Top Document 2 Cos_Sim 0.248:\n",
      "\n",
      "The Chego is a very small animal, about one fourth the size of a common\r\n",
      "flea: it is very troublesome, in warm climates, to the poor blacks, such\r\n",
      "as go barefoot, and the slovenly: it penetrates the skin, under which it\r\n",
      "lays a bunch of eggs, which swell to the bigness of a small pea.\n",
      "\n",
      "\n",
      "Top Document 3 Cos_Sim 0.185:\n",
      "\n",
      "\r\n",
      "This is one of the largest of the insect tribe. It is met with in\r\n",
      "different countries, and of various sizes, from two or three inches to\r\n",
      "nearly a foot in length: it somewhat resembles a lobster, and casts its\r\n",
      "skin, as the lobster does its shell.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_documents = documents[top_scores.sort().indices[0][-3:]][::-1]\n",
    "top_cosine_sim = list(top_scores.sort().values[0][-3:])[::-1]\n",
    "\n",
    "print(f'Question: {QUESTION}')\n",
    "\n",
    "for i, (cos_sim, top_document) in enumerate(zip(top_cosine_sim, top_documents)):\n",
    "    print(f'Top Document {i + 1} Cos_Sim {cos_sim:.3f}:\\n\\n{top_document}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e32cdde7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8524739146232605, 'start': 259, 'end': 262, 'answer': 'two'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer the question from the top document\n",
    "nlp(QUESTION, str(top_documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "901b68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is called an \"Open Book Q/A System\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f376c370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset adversarial_qa (/Users/sinanozdemir/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n"
     ]
    }
   ],
   "source": [
    "# load up the adversarial_qa dataset from the Q/A use-case\n",
    "training_qa = load_dataset('adversarial_qa', 'adversarialQA', split='train')\n",
    "\n",
    "good_training_data = []\n",
    "bad_training_data = []\n",
    "    \n",
    "last_example = None\n",
    "for example in training_qa:\n",
    "    if last_example and example['context'] != last_example['context']:\n",
    "        bad_training_data.append((example['question'], last_example['context'], 0))  #  add bad examples\n",
    "    # question, context, label is 1 if should be matched together\n",
    "    good_training_data.append((example['question'], example['context'], 1.0))\n",
    "    last_example = example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40ca4c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2647)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_training_data), len(bad_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "441fddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sbert.net/docs/training/overview.html for information on training\n",
    "\n",
    "seed(42)  # seed our upcoming sample\n",
    "\n",
    "sampled_training_data = sample(good_training_data, 200) + sample(bad_training_data, 200)\n",
    "\n",
    "shuffle(sampled_training_data)  # shuffle our data around\n",
    "\n",
    "training_index = int(.8 * len(sampled_training_data))  # Get an 80/20 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a29bbe54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': '',\n",
       " 'texts': ('Who ranked IBM on greenness?',\n",
       "  'IBM has 12 research laboratories worldwide, bundled into IBM Research. As of 2013[update] the company held the record for most patents generated by a business for 22 consecutive years. Its employees have garnered five Nobel Prizes, six Turing Awards, ten National Medals of Technology and five National Medals of Science. Notable company inventions or developments include the automated teller machine (ATM), the floppy disk, the hard disk drive, the magnetic stripe card, the relational database, the Universal Product Code (UPC), the financial swap, the Fortran programming language, SABRE airline reservation system, dynamic random-access memory (DRAM), copper wiring in semiconductors, the silicon-on-insulator (SOI) semiconductor manufacturing process, and Watson artificial intelligence.'),\n",
       " 'label': 0.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the training examples\n",
    "train_examples = [InputExample(texts=t[:2], label=t[2]) for t in sampled_training_data[:training_index]]\n",
    "\n",
    "train_examples[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d22f57f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
    "\n",
    "train_loss = losses.CosineSimilarityLoss(sbert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2af814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation data\n",
    "sentences1, sentences2, scores = zip(*sampled_training_data[training_index:])\n",
    "\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8269f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3eb2ef1fb0d42ac830edbc7d6f06507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d83c6f40914a5d8f7243ef7f839a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tune the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msbert_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mir/results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msampled_training_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:712\u001b[0m, in \u001b[0;36mSentenceTransformer.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    710\u001b[0m     skip_scheduler \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mget_scale() \u001b[38;5;241m!=\u001b[39m scale_before_step\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 712\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mloss_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m     loss_value\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    714\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(loss_model\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/sentence_transformers/losses/CosineSimilarityLoss.py:39\u001b[0m, in \u001b[0;36mCosineSimilarityLoss.forward\u001b[0;34m(self, sentence_features, labels)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor):\n\u001b[0;32m---> 39\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(sentence_feature)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[1;32m     40\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcos_score_transformation(torch\u001b[38;5;241m.\u001b[39mcosine_similarity(embeddings[\u001b[38;5;241m0\u001b[39m], embeddings[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fct(output, labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/sentence_transformers/losses/CosineSimilarityLoss.py:39\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor):\n\u001b[0;32m---> 39\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_feature\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[1;32m     40\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcos_score_transformation(torch\u001b[38;5;241m.\u001b[39mcosine_similarity(embeddings[\u001b[38;5;241m0\u001b[39m], embeddings[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fct(output, labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:551\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:327\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    325\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 327\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:271\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    280\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:216\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 216\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    217\u001b[0m context \u001b[38;5;241m=\u001b[39m unshape(context)  \u001b[38;5;66;03m# (bs, q_length, dim)\u001b[39;00m\n\u001b[1;32m    218\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_lin(context)  \u001b[38;5;66;03m# (bs, q_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tune the model\n",
    "sbert_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)], \n",
    "    output_path='ir/results',\n",
    "    epochs=1,\n",
    "#     warmup_steps=len(sampled_training_data) // 5, \n",
    "    evaluator=evaluator, \n",
    "    evaluation_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6ec94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cefe5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fine-tuned IR model\n",
    "finetuned_sbert_model = SentenceTransformer('ir/results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da880116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Document 1 Cos_Sim 0.496:\n",
      "\n",
      "When examined by a microscope, the flea is a pleasant object. The body\r\n",
      "is curiously adorned with a suit of polished armour, neatly jointed, and\r\n",
      "beset with a great number of sharp pins almost like the quills of a\r\n",
      "porcupine: it has a small head, large eyes, two horns, or feelers, which\r\n",
      "proceed from the head, and four long legs from the breast; they are very\r\n",
      "hairy and long, and have several joints, which fold as it were one\r\n",
      "within another.\n",
      "\n",
      "\n",
      "Top Document 2 Cos_Sim 0.253:\n",
      "\n",
      "The Chego is a very small animal, about one fourth the size of a common\r\n",
      "flea: it is very troublesome, in warm climates, to the poor blacks, such\r\n",
      "as go barefoot, and the slovenly: it penetrates the skin, under which it\r\n",
      "lays a bunch of eggs, which swell to the bigness of a small pea.\n",
      "\n",
      "\n",
      "Top Document 3 Cos_Sim 0.190:\n",
      "\n",
      "\r\n",
      "This is one of the largest of the insect tribe. It is met with in\r\n",
      "different countries, and of various sizes, from two or three inches to\r\n",
      "nearly a foot in length: it somewhat resembles a lobster, and casts its\r\n",
      "skin, as the lobster does its shell.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# re-encode the documents and run the same question as before\n",
    "document_embeddings = finetuned_sbert_model.encode(documents)\n",
    "\n",
    "query_embedding = finetuned_sbert_model.encode(QUESTION)  # embed the query into a vector space\n",
    "\n",
    "top_scores = util.cos_sim(query_embedding, document_embeddings)  # use cosine similarity to find the most relevant document\n",
    "\n",
    "top_documents = documents[top_scores.sort().indices[0][-3:]][::-1]\n",
    "top_cosine_sim = list(top_scores.sort().values[0][-3:])[::-1]\n",
    "\n",
    "print(f'Question: {QUESTION}')\n",
    "\n",
    "for i, (cos_sim, top_document) in enumerate(zip(top_cosine_sim, top_documents)):\n",
    "    print(f'Top Document {i + 1} Cos_Sim {cos_sim:.3f}:\\n\\n{top_document}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552b97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7585813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gutenberg_to_documents(gutenberg_url, sbert_model):\n",
    "    text = urlopen(gutenberg_url).read().decode()\n",
    "    documents = np.array(list(filter(lambda x: len(x) > 50, text.split('\\r\\n\\r\\n'))))\n",
    "    print(f'There are {len(documents)} documents/paragraphs')\n",
    "    return documents, sbert_model.encode(documents)\n",
    "\n",
    "\n",
    "def retrieve_relevant_documents(sbert_model, query, documents, embeddings, qa=None):\n",
    "    query_embedding = sbert_model.encode(query)  # embed the query into a vector space\n",
    "\n",
    "    top_scores = util.cos_sim(query_embedding, embeddings)  # use cosine similarity to find the most relevant document\n",
    "    top_documents = documents[top_scores.sort().indices[0][-3:]][::-1]\n",
    "    top_cosine_sim = list(top_scores.sort().values[0][-3:])[::-1]\n",
    "\n",
    "    for i, (cos_sim, top_document) in enumerate(zip(top_cosine_sim, top_documents)):\n",
    "        print(f'Top Document {i + 1} Cos_Sim {cos_sim:.3f}:\\n\\n{top_document}')\n",
    "        if qa:\n",
    "            answer = qa(question=query, context=top_document)\n",
    "            print(f'\\nAnswer: {answer}\\n')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc99564c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "banks_to_bassoon_documents, banks_to_bassoon_embeddings = gutenberg_to_documents(\n",
    "    'https://www.gutenberg.org/cache/epub/27480/pg27480.txt', finetuned_sbert_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_relevant_documents(finetuned_sbert_model,\n",
    "    'What is a banshee?', banks_to_bassoon_documents, banks_to_bassoon_embeddings,\n",
    "    nlp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3931a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_relevant_documents(finetuned_sbert_model,\n",
    "    'How do bassoons work?', banks_to_bassoon_documents, banks_to_bassoon_embeddings,\n",
    "    nlp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53187d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193f2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de5138c6",
   "metadata": {},
   "source": [
    "## 9.2 Teaching GPT multiple tasks at once with prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f0e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, GPT2LMHeadModel, pipeline, \\\n",
    "                         GPT2Tokenizer\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463affd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/snap/amazon-fine-food-reviews?select=Reviews.csv\n",
    "\n",
    "reviews = pd.read_csv('../data/reviews.csv')\n",
    "\n",
    "print(reviews.shape)\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Text'].str.len().plot(kind='hist', title='Histogram of Review Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d355929",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Summary'].str.len().plot(kind='hist', title='Histogram of Summary Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove very short and very long summaries\n",
    "reviews = reviews[(reviews['Summary'].str.len() >= 10) & (reviews['Summary'].str.len() < 25)]\n",
    "\n",
    "reviews['Summary'].str.len().plot(kind='hist', title='Histogram of Summary Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a4b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['Sentiment'] = reviews['Score'].map(lambda x: 'positive' if x >= 4 else 'neutral' if x == 3 else 'negative')\n",
    "\n",
    "reviews = reviews.groupby('Sentiment', group_keys=False).apply(lambda x: x.sample(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1936d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc72e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL = 'distilgpt2'\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#add two prompts, one for each task\n",
    "SENTIMENT_PROMPT = 'Sentiment Task'\n",
    "SUMMARIZE_PROMPT = 'Summarize Task'\n",
    "SENTIMENT_TOKEN = '\\nSentiment:'\n",
    "SUMMARIZE_TOKEN = '\\nSummarize:'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690adf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['sentiment_text'] = f'{SENTIMENT_PROMPT}\\nReview: ' + reviews['Text'] + SENTIMENT_TOKEN +  ' ' + reviews['Sentiment'].astype(str)\n",
    "\n",
    "reviews['summarize_text'] = f'{SUMMARIZE_PROMPT}\\nReview: ' + reviews['Text'] + SUMMARIZE_TOKEN +  ' ' + reviews['Summary'].astype(str)\n",
    "\n",
    "reviews['sentiment_text'].head(2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d804737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['summarize_text'].head(2).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657237e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.sample(frac=1)\n",
    "\n",
    "training_examples = reviews['summarize_text'].tolist() + reviews['sentiment_text'].tolist()\n",
    "\n",
    "print(len(training_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_task_df = pd.DataFrame({'text': training_examples})\n",
    "\n",
    "data = Dataset.from_pandas(multi_task_df)\n",
    "\n",
    "def preprocess(examples):\n",
    "    results = tokenizer(examples['text'], truncation=True)\n",
    "    return results\n",
    "\n",
    "data = data.map(preprocess, batched=True)\n",
    "\n",
    "data = data.train_test_split(train_size=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c356604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb033e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8b5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_multitask\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=5, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=32,  # batch size for evaluation\n",
    "    warmup_steps=len(data['train']) // 10,  # number of warmup steps for learning rate scheduler,\n",
    "    weight_decay = 0.02,\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps'\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['test'],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea18d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656986c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = GPT2LMHeadModel.from_pretrained('./gpt2_multitask')\n",
    "\n",
    "generator = pipeline('text-generation', model=loaded_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549ba9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e48142",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample, score, summary = reviews.sample(1)[['Text', 'Sentiment', 'Summary']].values[0]\n",
    "\n",
    "print(text_sample)\n",
    "print(score)\n",
    "print(summary)\n",
    "\n",
    "num_tokens = len(tokenizer(text_sample)['input_ids'])\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_text_sample = f'{SENTIMENT_PROMPT}\\nReview: {text_sample}{SENTIMENT_TOKEN}'\n",
    "summarize_text_sample = f'{SUMMARIZE_PROMPT}\\nReview: {text_sample}{SUMMARIZE_TOKEN}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687eb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "for generated_text in generator(sentiment_text_sample, num_return_sequences=3, max_length=num_tokens + 1):\n",
    "    print(generated_text['generated_text'])\n",
    "    print('----')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73126886",
   "metadata": {},
   "outputs": [],
   "source": [
    "for generated_text in generator(summarize_text_sample, num_return_sequences=3, max_length=num_tokens + 20):\n",
    "    print(generated_text['generated_text'])\n",
    "    print('----')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0452f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1f311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
