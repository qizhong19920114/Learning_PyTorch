{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b97d18c",
   "metadata": {},
   "source": [
    "## 6.1 Flavors of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55fc4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3af241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "90a1c04d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /Users/sinanozdemir/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n",
      "If you don’t *** at the sign, you will get a ticket\n",
      "Token:look. Score: 51.35%\n",
      "Token:stop. Score: 39.66%\n",
      "Token:glance. Score: 1.02%\n",
      "Token:wait. Score: 0.60%\n",
      "Token:turn. Score: 0.57%\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model='bert-base-cased')\n",
    "\n",
    "print(type(nlp.model))\n",
    "\n",
    "preds = nlp(f\"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print('If you don’t *** at the sign, you will get a ticket')\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f192f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1c993940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /Users/sinanozdemir/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForMaskedLM'>\n",
      "If you don’t *** at the sign, you will get a ticket\n",
      "Token: look. Score: 47.69%\n",
      "Token: stop. Score: 36.82%\n",
      "Token: stand. Score: 2.54%\n",
      "Token: stay. Score: 2.52%\n",
      "Token: wave. Score: 1.01%\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model='roberta-base')\n",
    "\n",
    "print(type(nlp.model))\n",
    "\n",
    "preds = nlp(f\"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print('If you don’t *** at the sign, you will get a ticket')\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "81cceeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /Users/sinanozdemir/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"distilroberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.roberta.modeling_roberta.RobertaForMaskedLM'>\n",
      "If you don’t *** at the sign, you will get a ticket\n",
      "Token: stop. Score: 42.11%\n",
      "Token: look. Score: 7.53%\n",
      "Token: park. Score: 4.92%\n",
      "Token: arrive. Score: 4.65%\n",
      "Token: sign. Score: 4.27%\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model='distilroberta-base')\n",
    "\n",
    "print(type(nlp.model))\n",
    "\n",
    "preds = nlp(f\"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print('If you don’t *** at the sign, you will get a ticket')\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfa6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6dbf2ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "All model checkpoint weights were used when initializing DistilBertForMaskedLM.\n",
      "\n",
      "All the weights of DistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForMaskedLM for predictions without further training.\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForMaskedLM'>\n",
      "If you don’t *** at the sign, you will get a ticket\n",
      "Token:look. Score: 57.47%\n",
      "Token:stop. Score: 7.37%\n",
      "Token:glance. Score: 3.74%\n",
      "Token:arrive. Score: 2.16%\n",
      "Token:appear. Score: 1.87%\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"fill-mask\", model='distilbert-base-cased')  # Using a flavor of BERT called DistilBERT\n",
    "\n",
    "print(type(nlp.model))  \n",
    "\n",
    "preds = nlp(f\"If you don’t {nlp.tokenizer.mask_token} at the sign, you will get a ticket\")\n",
    "\n",
    "print('If you don’t *** at the sign, you will get a ticket')\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64a7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9098dcb1",
   "metadata": {},
   "source": [
    "## 6.2 BERT for sequence classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2484167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification, DistilBertTokenizerFast, \\\n",
    "     DataCollatorWithPadding, pipeline\n",
    "from datasets import load_metric, Dataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac898e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e4e17957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'listen O\\r\\n',\n",
       " b'to O\\r\\n',\n",
       " b'westbam B-artist\\r\\n',\n",
       " b'alumb O\\r\\n',\n",
       " b'allergic B-album\\r\\n',\n",
       " b'on O\\r\\n',\n",
       " b'google B-service\\r\\n',\n",
       " b'music I-service\\r\\n',\n",
       " b'PlayMusic\\r\\n',\n",
       " b'\\r\\n',\n",
       " b'add O\\r\\n',\n",
       " b'step B-entity_name\\r\\n',\n",
       " b'to I-entity_name\\r\\n',\n",
       " b'me I-entity_name\\r\\n',\n",
       " b'to O\\r\\n',\n",
       " b'the O\\r\\n',\n",
       " b'50 B-playlist\\r\\n',\n",
       " b'cl\\xc3\\xa1sicos I-playlist\\r\\n',\n",
       " b'playlist O\\r\\n',\n",
       " b'AddToPlaylist\\r\\n']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_file = open('../data/snips.train.txt', 'rb')\n",
    "\n",
    "snips_rows = snips_file.readlines()\n",
    "\n",
    "snips_rows[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9f7b8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code segment parses the snips dataset into a more manageable format\n",
    "\n",
    "utterances = []\n",
    "tokenized_utterances = []\n",
    "labels_for_tokens = []\n",
    "sequence_labels = []\n",
    "\n",
    "utterance, tokenized_utterance, label_for_utterances = '', [], []\n",
    "for snip_row in snips_rows:\n",
    "    if len(snip_row) == 2:  # skip over rows with no data\n",
    "        continue\n",
    "    if ' ' not in snip_row.decode():  # we've hit a sequence label\n",
    "        sequence_labels.append(snip_row.decode().strip())\n",
    "        utterances.append(utterance.strip())\n",
    "        tokenized_utterances.append(tokenized_utterance)\n",
    "        labels_for_tokens.append(label_for_utterances)\n",
    "        utterance = ''\n",
    "        tokenized_utterance = []\n",
    "        label_for_utterances = []\n",
    "        continue\n",
    "    token, token_label = snip_row.decode().split(' ')\n",
    "    token_label = token_label.strip()\n",
    "    utterance += f'{token} '\n",
    "    tokenized_utterance.append(token)\n",
    "    label_for_utterances.append(token_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c78793ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13084, 13084, 13084, 13084)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_for_tokens), len(tokenized_utterances), len(utterances), len(sequence_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "774c7c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bb5d631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PlayMusic',\n",
       " 'RateBook',\n",
       " 'SearchScreeningEvent',\n",
       " 'SearchCreativeWork',\n",
       " 'BookRestaurant',\n",
       " 'GetWeather',\n",
       " 'AddToPlaylist']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sequence_labels = list(set(sequence_labels))\n",
    "unique_sequence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3e9fce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 unique sequence labels\n"
     ]
    }
   ],
   "source": [
    "sequence_labels = [unique_sequence_labels.index(l) for l in sequence_labels]\n",
    "\n",
    "print(f'There are {len(unique_sequence_labels)} unique sequence labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6de12c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 72 unique token labels\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "unique_token_labels = list(set(reduce(lambda x, y: x + y, labels_for_tokens)))\n",
    "labels_for_tokens = [[unique_token_labels.index(_) for _ in l] for l in labels_for_tokens]\n",
    "\n",
    "print(f'There are {len(unique_token_labels)} unique token labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3a5251b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['listen', 'to', 'westbam', 'alumb', 'allergic', 'on', 'google', 'music']\n",
      "[22, 22, 68, 22, 58, 22, 51, 4]\n",
      "['O', 'O', 'B-artist', 'O', 'B-album', 'O', 'B-service', 'I-service']\n",
      "listen to westbam alumb allergic on google music\n",
      "0\n",
      "PlayMusic\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_utterances[0])\n",
    "print(labels_for_tokens[0])\n",
    "print([unique_token_labels[l] for l in labels_for_tokens[0]])\n",
    "print(utterances[0])\n",
    "print(sequence_labels[0])\n",
    "print(unique_sequence_labels[sequence_labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "eec863af",
   "metadata": {},
   "outputs": [],
   "source": [
    "snips_dataset = Dataset.from_dict(\n",
    "    dict(\n",
    "        utterance=utterances, \n",
    "        label=sequence_labels,\n",
    "        tokens=tokenized_utterances,\n",
    "        token_labels=labels_for_tokens\n",
    "    )\n",
    ")\n",
    "snips_dataset = snips_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "67a6b70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'rate homicide: a year on the killing streets five stars',\n",
       " 'label': 1,\n",
       " 'tokens': ['rate',\n",
       "  'homicide:',\n",
       "  'a',\n",
       "  'year',\n",
       "  'on',\n",
       "  'the',\n",
       "  'killing',\n",
       "  'streets',\n",
       "  'five',\n",
       "  'stars'],\n",
       " 'token_labels': [22, 44, 13, 13, 13, 13, 13, 13, 53, 29]}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc13ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to batch tokenize utterances with truncation\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"utterance\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bee6381f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383065db38084bd1a0cec52c5f3dd274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170c76177e9f46f498d31d1a85d2edd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_clf_tokenized_snips = snips_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3da2a0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'play a twenties song',\n",
       " 'label': 0,\n",
       " 'tokens': ['play', 'a', 'twenties', 'song'],\n",
       " 'token_labels': [22, 22, 37, 7],\n",
       " 'input_ids': [101, 1505, 170, 21708, 1461, 102],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_clf_tokenized_snips['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a2333ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCollatorWithPadding creates batch of data. It also dynamically pads text to the \n",
    "#  length of the longest element in the batch, making them all the same length. \n",
    "#  It's possible to pad your text in the tokenizer function with padding=True, dynamic padding is more efficient.\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54954b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b14125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator will pad data so that all examples are the same input length.\n",
    "#  Attention mask is how we ignore attention scores for padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154039ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d647d950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sequence_clf_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-cased', \n",
    "    num_labels=len(unique_sequence_labels),\n",
    ")\n",
    "\n",
    "# set an index -> label dictionary\n",
    "sequence_clf_model.config.id2label = {i: l for i, l in enumerate(unique_sequence_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4db4d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PlayMusic'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_clf_model.config.id2label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6352abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):  # custom method to take in logits and calculate accuracy of the eval set\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a04da07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./snips_clf/results\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # some deep learning parameters that the Trainer is able to take in\n",
    "    warmup_steps=len(seq_clf_tokenized_snips['train']) // 5,  # number of warmup steps for learning rate scheduler,\n",
    "    weight_decay = 0.05,\n",
    "    \n",
    "    logging_steps=1,\n",
    "    log_level='info',\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=50,\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "# Define the trainer:\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=sequence_clf_model,\n",
    "    args=training_args,\n",
    "    train_dataset=seq_clf_tokenized_snips['train'],\n",
    "    eval_dataset=seq_clf_tokenized_snips['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1c7d229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='164' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 12:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9550352096557617,\n",
       " 'eval_accuracy': 0.09973251815055408,\n",
       " 'eval_runtime': 64.0129,\n",
       " 'eval_samples_per_second': 40.882,\n",
       " 'eval_steps_per_second': 1.281}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get initial metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08f1cf8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running training *****\n",
      "  Num examples = 10467\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 656\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='656' max='656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [656/656 22:53, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.159812</td>\n",
       "      <td>0.971341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.051554</td>\n",
       "      <td>0.987008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./snips_clf/results/checkpoint-328\n",
      "Configuration saved in ./snips_clf/results/checkpoint-328/config.json\n",
      "Model weights saved in ./snips_clf/results/checkpoint-328/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./snips_clf/results/checkpoint-656\n",
      "Configuration saved in ./snips_clf/results/checkpoint-656/config.json\n",
      "Model weights saved in ./snips_clf/results/checkpoint-656/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./snips_clf/results/checkpoint-656 (score: 0.05155394226312637).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=656, training_loss=0.6844381606277283, metrics={'train_runtime': 1376.1424, 'train_samples_per_second': 15.212, 'train_steps_per_second': 0.477, 'total_flos': 131576695874496.0, 'train_loss': 0.6844381606277283, 'epoch': 2.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8fe9537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.05155394226312637,\n",
       " 'eval_accuracy': 0.9870080244554834,\n",
       " 'eval_runtime': 51.0558,\n",
       " 'eval_samples_per_second': 51.258,\n",
       " 'eval_steps_per_second': 1.606,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e93471e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'AddToPlaylist', 'score': 0.9956121444702148}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", sequence_clf_model, tokenizer=tokenizer)\n",
    "pipe('Please add Here We Go by Dispatch to my road trip playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b373004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./snips_clf/results\n",
      "Configuration saved in ./snips_clf/results/config.json\n",
      "Model weights saved in ./snips_clf/results/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d7d27bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./snips_clf/results/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./snips_clf/results\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"PlayMusic\",\n",
      "    \"1\": \"RateBook\",\n",
      "    \"2\": \"SearchScreeningEvent\",\n",
      "    \"3\": \"SearchCreativeWork\",\n",
      "    \"4\": \"BookRestaurant\",\n",
      "    \"5\": \"GetWeather\",\n",
      "    \"6\": \"AddToPlaylist\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file ./snips_clf/results/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./snips_clf/results\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"PlayMusic\",\n",
      "    \"1\": \"RateBook\",\n",
      "    \"2\": \"SearchScreeningEvent\",\n",
      "    \"3\": \"SearchCreativeWork\",\n",
      "    \"4\": \"BookRestaurant\",\n",
      "    \"5\": \"GetWeather\",\n",
      "    \"6\": \"AddToPlaylist\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./snips_clf/results/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./snips_clf/results.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'AddToPlaylist', 'score': 0.9956121444702148}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\", \"./snips_clf/results\", tokenizer=tokenizer)\n",
    "pipe('Please add Here We Go by Dispatch to my road trip playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15302def",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "52fef242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "frozen_sequence_clf_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-cased', \n",
    "    num_labels=len(unique_sequence_labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e8fa35be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in frozen_sequence_clf_model.distilbert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8dad2bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./snips_clf/results\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # some deep learning parameters that the Trainer is able to take in\n",
    "    warmup_steps=len(seq_clf_tokenized_snips['train']) // 5,  # number of warmup steps for learning rate scheduler,\n",
    "    weight_decay = 0.05,\n",
    "    \n",
    "    logging_steps=1,\n",
    "    log_level='info',\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=50,\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "# Define the trainer:\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=frozen_sequence_clf_model,\n",
    "    args=training_args,\n",
    "    train_dataset=seq_clf_tokenized_snips['train'],\n",
    "    eval_dataset=seq_clf_tokenized_snips['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cc1d2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='164' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 03:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9511256217956543,\n",
       " 'eval_accuracy': 0.13870844478410393,\n",
       " 'eval_runtime': 33.0914,\n",
       " 'eval_samples_per_second': 79.084,\n",
       " 'eval_steps_per_second': 2.478}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bfa45688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running training *****\n",
      "  Num examples = 10467\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 656\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='656' max='656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [656/656 06:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.894000</td>\n",
       "      <td>1.889941</td>\n",
       "      <td>0.356133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.652300</td>\n",
       "      <td>1.679589</td>\n",
       "      <td>0.872755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./snips_clf/results/checkpoint-328\n",
      "Configuration saved in ./snips_clf/results/checkpoint-328/config.json\n",
      "Model weights saved in ./snips_clf/results/checkpoint-328/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./snips_clf/results/checkpoint-656\n",
      "Configuration saved in ./snips_clf/results/checkpoint-656/config.json\n",
      "Model weights saved in ./snips_clf/results/checkpoint-656/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./snips_clf/results/checkpoint-656 (score: 1.679589033126831).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=656, training_loss=1.8726866072634372, metrics={'train_runtime': 377.717, 'train_samples_per_second': 55.422, 'train_steps_per_second': 1.737, 'total_flos': 131576695874496.0, 'train_loss': 1.8726866072634372, 'epoch': 2.0})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()  # ~23min -> ~6min on my laptop with all of distilbert frozen with a worse loss/accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b911ff29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_labels, tokens, utterance.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.679589033126831,\n",
       " 'eval_accuracy': 0.8727550630492931,\n",
       " 'eval_runtime': 32.8797,\n",
       " 'eval_samples_per_second': 79.593,\n",
       " 'eval_steps_per_second': 2.494,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b3b0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d99579a",
   "metadata": {},
   "source": [
    "## 6.3 BERT for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb6edfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification, DistilBertForTokenClassification, \\\n",
    "                         DistilBertTokenizerFast, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3cdb1bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/vocab.txt from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ba377304984dc63e3ede0e23a938bbbf04d5c3835b66d5bb48343aecca188429.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/acb5c2138c1f8c84f074b86dafce3631667fccd6efcb1a7ea1320cf75c386a36.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-cased/resolve/main/tokenizer_config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/81e970e5e6ec68be12da0f8f3b2f2469c78d579282299a2ea65b4b7441719107.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using a cased tokenizer because I think case will matter\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "446479c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'play a twenties song',\n",
       " 'label': 0,\n",
       " 'tokens': ['play', 'a', 'twenties', 'song'],\n",
       " 'token_labels': [22, 22, 37, 7]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e652f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "36d41d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The given \"token_labels\" may not match up with the BERT wordpiece tokenization so\n",
    "#  this function will map them to the tokenization that BERT uses\n",
    "#  -100 is a reserved for labels where we do not want to calculate losses so BERT doesn't waste time\n",
    "#  trying to predict tokens like CLS or SEP\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"token_labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:  # Set the special tokens to -100.\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)  # CLS and SEP are labeled as -100\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "50a7ac1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'play a twenties song',\n",
       " 'label': 0,\n",
       " 'tokens': ['play', 'a', 'twenties', 'song'],\n",
       " 'token_labels': [22, 22, 37, 7]}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "832dd8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13ff442399a4a7daf54f746012dde79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b5c3598b0c40ebb97545a02faa2e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# map our dataset from sequence classification to be for token classification\n",
    "tok_clf_tokenized_snips = snips_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d4b31d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'play a twenties song',\n",
       " 'label': 0,\n",
       " 'tokens': ['play', 'a', 'twenties', 'song'],\n",
       " 'token_labels': [22, 22, 37, 7],\n",
       " 'input_ids': [101, 1505, 170, 21708, 1461, 102],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 22, 22, 37, 7, -100]}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_clf_tokenized_snips['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f4b4c9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10467\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2617\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_clf_tokenized_snips['train'] = tok_clf_tokenized_snips['train'].remove_columns(\n",
    "    ['utterance', 'label', 'tokens', 'token_labels']\n",
    ")\n",
    "\n",
    "tok_clf_tokenized_snips['test'] = tok_clf_tokenized_snips['test'].remove_columns(\n",
    "    ['utterance', 'label', 'tokens', 'token_labels']\n",
    ")\n",
    "\n",
    "tok_clf_tokenized_snips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1f29865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b715cf3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-cased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/ebe1ea24d11aa664488b8de5b21e33989008ca78f207d4e30ec6350b693f073f.302bfd1b5e031cc1b17796e0b6e5b242ba2045d31d00f97589e12b458ebff27a\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-cased/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tok_clf_model = DistilBertForTokenClassification.from_pretrained(\n",
    "    'distilbert-base-cased', num_labels=len(unique_token_labels)\n",
    ")\n",
    "\n",
    "# Set our label dictionary\n",
    "tok_clf_model.config.id2label = {i: l for i, l in enumerate(unique_token_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "08bda49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('B-country', 'B-timeRange')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_clf_model.config.id2label[0], tok_clf_model.config.id2label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c848c3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./snips_tok_clf/results\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    load_best_model_at_end=True,\n",
    "        \n",
    "    logging_steps=10,\n",
    "    log_level='info',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "# Define the trainer:\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=tok_clf_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tok_clf_tokenized_snips['train'],\n",
    "    eval_dataset=tok_clf_tokenized_snips['test'],\n",
    "    data_collator=tok_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3c1c1b92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='164' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 17:52]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.45733642578125,\n",
       " 'eval_runtime': 40.4286,\n",
       " 'eval_samples_per_second': 64.731,\n",
       " 'eval_steps_per_second': 2.028}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4f9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f6cd1c90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 10467\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 656\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='656' max='656' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [656/656 30:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.172545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.130660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./snips_tok_clf/results/checkpoint-328\n",
      "Configuration saved in ./snips_tok_clf/results/checkpoint-328/config.json\n",
      "Model weights saved in ./snips_tok_clf/results/checkpoint-328/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./snips_tok_clf/results/checkpoint-656\n",
      "Configuration saved in ./snips_tok_clf/results/checkpoint-656/config.json\n",
      "Model weights saved in ./snips_tok_clf/results/checkpoint-656/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./snips_tok_clf/results/checkpoint-656 (score: 0.13066013157367706).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=656, training_loss=0.397148780070427, metrics={'train_runtime': 1851.7923, 'train_samples_per_second': 11.305, 'train_steps_per_second': 0.354, 'total_flos': 129927264993792.0, 'train_loss': 0.397148780070427, 'epoch': 2.0})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "01b18ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2617\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.13066013157367706,\n",
       " 'eval_runtime': 34.3515,\n",
       " 'eval_samples_per_second': 76.183,\n",
       " 'eval_steps_per_second': 2.387,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8fb6a994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-entity_name',\n",
       "  'score': 0.887767,\n",
       "  'index': 3,\n",
       "  'word': 'Here',\n",
       "  'start': 11,\n",
       "  'end': 15},\n",
       " {'entity': 'I-entity_name',\n",
       "  'score': 0.88551474,\n",
       "  'index': 4,\n",
       "  'word': 'We',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-entity_name',\n",
       "  'score': 0.9170048,\n",
       "  'index': 5,\n",
       "  'word': 'Go',\n",
       "  'start': 19,\n",
       "  'end': 21},\n",
       " {'entity': 'B-artist',\n",
       "  'score': 0.93062943,\n",
       "  'index': 7,\n",
       "  'word': 'Di',\n",
       "  'start': 25,\n",
       "  'end': 27},\n",
       " {'entity': 'I-artist',\n",
       "  'score': 0.94451386,\n",
       "  'index': 8,\n",
       "  'word': '##sp',\n",
       "  'start': 27,\n",
       "  'end': 29},\n",
       " {'entity': 'I-artist',\n",
       "  'score': 0.78699875,\n",
       "  'index': 9,\n",
       "  'word': '##atch',\n",
       "  'start': 29,\n",
       "  'end': 33},\n",
       " {'entity': 'B-playlist_owner',\n",
       "  'score': 0.9935272,\n",
       "  'index': 11,\n",
       "  'word': 'my',\n",
       "  'start': 37,\n",
       "  'end': 39},\n",
       " {'entity': 'B-playlist',\n",
       "  'score': 0.994918,\n",
       "  'index': 12,\n",
       "  'word': 'road',\n",
       "  'start': 40,\n",
       "  'end': 44},\n",
       " {'entity': 'I-playlist',\n",
       "  'score': 0.9942649,\n",
       "  'index': 13,\n",
       "  'word': 'trip',\n",
       "  'start': 45,\n",
       "  'end': 49}]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"token-classification\", tok_clf_model, tokenizer=tokenizer)\n",
    "pipe('Please add Here We Go by Dispatch to my road trip playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6749678a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-object_name',\n",
       "  'score': 0.94716674,\n",
       "  'index': 2,\n",
       "  'word': 'the',\n",
       "  'start': 5,\n",
       "  'end': 8},\n",
       " {'entity': 'I-object_name',\n",
       "  'score': 0.95756745,\n",
       "  'index': 3,\n",
       "  'word': 'do',\n",
       "  'start': 9,\n",
       "  'end': 11},\n",
       " {'entity': 'I-object_name',\n",
       "  'score': 0.9812471,\n",
       "  'index': 4,\n",
       "  'word': '##og',\n",
       "  'start': 11,\n",
       "  'end': 13},\n",
       " {'entity': 'I-object_name',\n",
       "  'score': 0.97237736,\n",
       "  'index': 5,\n",
       "  'word': 'food',\n",
       "  'start': 14,\n",
       "  'end': 18},\n",
       " {'entity': 'B-rating_value',\n",
       "  'score': 0.9964361,\n",
       "  'index': 6,\n",
       "  'word': '5',\n",
       "  'start': 19,\n",
       "  'end': 20},\n",
       " {'entity': 'B-best_rating',\n",
       "  'score': 0.97492224,\n",
       "  'index': 9,\n",
       "  'word': '5',\n",
       "  'start': 28,\n",
       "  'end': 29}]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"token-classification\", tok_clf_model, tokenizer=tokenizer)\n",
    "pipe('Rate the doog food 5 out of 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235ced5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09ac8a2a",
   "metadata": {},
   "source": [
    "## 6.4 BERT for question/answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb67c5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, pipeline\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a8066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81682de5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-large-uncased/resolve/main/vocab.txt from cache at /Users/sinanozdemir/.cache/huggingface/transformers/e12f02d630da91a0982ce6db1ad595231d155a2b725ab106971898276d842ecc.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-large-uncased/resolve/main/tokenizer.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/475d46024228961ca8770cead39e1079f135fd2441d14cf216727ffac8d41d78.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-large-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-large-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-large-uncased/resolve/main/tokenizer_config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/300ecd79785b4602752c0085f8a89c3f0232ef367eda291c79a5600f3778b677.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-large-uncased/resolve/main/config.json from cache at /Users/sinanozdemir/.cache/huggingface/transformers/1cf090f220f9674b67b3434decfe4d40a6532d7849653eac435ff94d31a4904c.1d03e5e4fa2db2532c517b2cd98290d8444b237619bd3d2039850a6d5e86473d\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-large-uncased/resolve/main/pytorch_model.bin from cache at /Users/sinanozdemir/.cache/huggingface/transformers/1d959166dd7e047e57ea1b2d9b7b9669938a7e90c5e37a03961ad9f15eaea17f.fea64cd906e3766b04c92397f9ad3ff45271749cbe49829a079dd84e34c1697d\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)\n",
    "\n",
    "qa_bert = BertForQuestionAnswering.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e17501e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset adversarial_qa (/Users/sinanozdemir/.cache/huggingface/datasets/adversarial_qa/adversarialQA/1.0.0/92356be07b087c5c6a543138757828b8d61ca34de8a87807d40bbc0e6c68f04b)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# This code snippet parses a dataset from Huggingface to give the four parameters we need for Q/A:\n",
    "#  question (str), context (str), start_positions (int), end_positions (int)\n",
    "\n",
    "q, c, s, e, f_a = [], [], [], [], []\n",
    "\n",
    "def get_sub_list_positions(context_encoded, answer_encoded):\n",
    "    for idx in range(len(context_encoded) - len(answer_encoded) + 1):\n",
    "        if context_encoded[idx : idx + len(answer_encoded)] == answer_encoded:\n",
    "            return idx, idx + len(answer_encoded) - 1\n",
    "    return None, None\n",
    "    \n",
    "for example in load_dataset('adversarial_qa', 'adversarialQA', split='train'):\n",
    "    context_encoded = bert_tokenizer.encode(example['question'], example['context'])\n",
    "    for answer, answer_start in zip(example['answers']['text'], example['answers']['answer_start']):\n",
    "        answer_encoded = bert_tokenizer.encode(answer, add_special_tokens=False)  # ignore the CLS and SEP token\n",
    "        start_pos, end_pos = get_sub_list_positions(context_encoded, answer_encoded)\n",
    "        if start_pos:\n",
    "            q.append(example['question'])\n",
    "            c.append(example['context'])\n",
    "            s.append(start_pos)\n",
    "            e.append(end_pos)\n",
    "            f_a.append(answer)\n",
    "            break\n",
    "    \n",
    "qa_df = pd.DataFrame({\n",
    "    'question': q, 'context': c, 'start_positions': s, 'end_positions': e, 'answer':  f_a\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9719c919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29989, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f65283a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>start_positions</th>\n",
       "      <th>end_positions</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What sare the benifts of the blood brain barrir?</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>56</td>\n",
       "      <td>60</td>\n",
       "      <td>isolated from the bloodstream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is surrounded by cerebrospinal fluid?</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the skull protect?</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What has been injected into rats to produce pr...</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>chemicals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What can cause issues with how the brain works?</td>\n",
       "      <td>Another approach to brain function is to exami...</td>\n",
       "      <td>93</td>\n",
       "      <td>94</td>\n",
       "      <td>brain damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29984</th>\n",
       "      <td>What is the lowest ISO mentioned?</td>\n",
       "      <td>Some high-speed black-and-white films, such as...</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29985</th>\n",
       "      <td>What is the highest ISO mentioned?</td>\n",
       "      <td>Some high-speed black-and-white films, such as...</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>3200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29986</th>\n",
       "      <td>What is Kodak'sbrand name of E6 film?</td>\n",
       "      <td>Some high-speed black-and-white films, such as...</td>\n",
       "      <td>124</td>\n",
       "      <td>126</td>\n",
       "      <td>Ektachrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29987</th>\n",
       "      <td>How do these films differ?</td>\n",
       "      <td>Some high-speed black-and-white films, such as...</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>high-speed black-and-white films</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29988</th>\n",
       "      <td>What letter designates what Ektachrome is desi...</td>\n",
       "      <td>Some high-speed black-and-white films, such as...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29989 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "0       What sare the benifts of the blood brain barrir?   \n",
       "1             What is surrounded by cerebrospinal fluid?   \n",
       "2                           What does the skull protect?   \n",
       "3      What has been injected into rats to produce pr...   \n",
       "4        What can cause issues with how the brain works?   \n",
       "...                                                  ...   \n",
       "29984                  What is the lowest ISO mentioned?   \n",
       "29985                 What is the highest ISO mentioned?   \n",
       "29986              What is Kodak'sbrand name of E6 film?   \n",
       "29987                         How do these films differ?   \n",
       "29988  What letter designates what Ektachrome is desi...   \n",
       "\n",
       "                                                 context  start_positions  \\\n",
       "0      Another approach to brain function is to exami...               56   \n",
       "1      Another approach to brain function is to exami...               16   \n",
       "2      Another approach to brain function is to exami...               11   \n",
       "3      Another approach to brain function is to exami...              153   \n",
       "4      Another approach to brain function is to exami...               93   \n",
       "...                                                  ...              ...   \n",
       "29984  Some high-speed black-and-white films, such as...              140   \n",
       "29985  Some high-speed black-and-white films, such as...               25   \n",
       "29986  Some high-speed black-and-white films, such as...              124   \n",
       "29987  Some high-speed black-and-white films, such as...                9   \n",
       "29988  Some high-speed black-and-white films, such as...               38   \n",
       "\n",
       "       end_positions                            answer  \n",
       "0                 60     isolated from the bloodstream  \n",
       "1                 16                             brain  \n",
       "2                 11                             brain  \n",
       "3                153                         chemicals  \n",
       "4                 94                      brain damage  \n",
       "...              ...                               ...  \n",
       "29984            140                               400  \n",
       "29985             26                              3200  \n",
       "29986            126                        Ektachrome  \n",
       "29987             17  high-speed black-and-white films  \n",
       "29988             38                                 P  \n",
       "\n",
       "[29989 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea9a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "645f97db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# only grab 4,000 examples\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m qa_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(\u001b[43mqa_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Dataset has a built in train test split method\u001b[39;00m\n\u001b[1;32m      6\u001b[0m qa_dataset[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/pandas/core/generic.py:5454\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   5451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5452\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[0;32m-> 5454\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5455\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   5457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/pandas/core/sample.py:150\u001b[0m, in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m    151\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    152\u001b[0m )\n",
      "File \u001b[0;32mmtrand.pyx:965\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "# only grab 4,000 examples\n",
    "qa_dataset = Dataset.from_pandas(qa_df.sample(4000, random_state=42))\n",
    "\n",
    "# Dataset has a built in train test split method\n",
    "\n",
    "qa_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c29b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91087a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will pad our dataset so that our input matrices are the same length and truncate anything longer than 512 tokens\n",
    "# This will not require a data collator\n",
    "def preprocess(data):\n",
    "    return bert_tokenizer(data['question'], data['context'], truncation=True)\n",
    "\n",
    "qa_dataset = qa_dataset.map(preprocess, batched=True, batch_size=512)\n",
    "\n",
    "qa_dataset = qa_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We use the token_type_ids to differentiate between sentence A and sentence B\n",
    "# qa_train.set_format(\n",
    "#     'torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "# )\n",
    "# qa_test.set_format(\n",
    "#     'torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca305573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa_train[0]  # See how the attention mask changs from 1s to 0s and how segment ids change from 0s to 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcf73c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all but the last 2 encoder layers in BERT to speed up training\n",
    "for param in list(qa_bert.bert.parameters())[:357]:\n",
    "    param.requires_grad = False  # disable training in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c8ebbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('encoder.layer.21.output.LayerNorm.bias',\n",
       " 'encoder.layer.22.attention.self.query.weight')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(qa_bert.bert.named_parameters())[356][0], list(qa_bert.bert.named_parameters())[357][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "006ba4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b7c6f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: __index_level_0__, question, context, answer.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 800\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/25 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(  \u001b[38;5;66;03m# We don't have a collator here because we did our padding in our preprocessing function\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mqa_bert,\n\u001b[1;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Get initial metrics\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/trainer.py:2158\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2155\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2157\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2158\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2162\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2168\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   2169\u001b[0m output\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m   2170\u001b[0m     speed_metrics(\n\u001b[1;32m   2171\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2175\u001b[0m     )\n\u001b[1;32m   2176\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/trainer.py:2332\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2329\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 2332\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[38;5;66;03m# Update containers on host\u001b[39;00m\n\u001b[1;32m   2335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/trainer.py:2537\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   2535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels:\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 2537\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2538\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/trainer.py:1923\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1923\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1839\u001b[0m, in \u001b[0;36mBertForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1827\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1828\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1837\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1839\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1851\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1853\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    989\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    990\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    991\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    995\u001b[0m )\n\u001b[0;32m--> 996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1009\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    578\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    583\u001b[0m     )\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:472\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    462\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m ):\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:402\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    394\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ):\n\u001b[0;32m--> 402\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    412\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:306\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    303\u001b[0m     past_key_value \u001b[38;5;241m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative_key_query\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    309\u001b[0m     seq_length \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./qa/results',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_dir='./qa/logs',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(  # We don't have a collator here because we did our padding in our preprocessing function\n",
    "    model=qa_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=qa_train,\n",
    "    eval_dataset=qa_test,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Get initial metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05e29d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: __index_level_0__, context, answer, question.\n",
      "***** Running training *****\n",
      "  Num examples = 3200\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='269' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [269/400 7:34:33 < 3:43:01, 0.01 it/s, Epoch 2.68/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.314100</td>\n",
       "      <td>4.248008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.009000</td>\n",
       "      <td>4.084617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: __index_level_0__, context, answer, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 800\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./qa/results/checkpoint-100\n",
      "Configuration saved in ./qa/results/checkpoint-100/config.json\n",
      "Model weights saved in ./qa/results/checkpoint-100/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: __index_level_0__, context, answer, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 800\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./qa/results/checkpoint-200\n",
      "Configuration saved in ./qa/results/checkpoint-200/config.json\n",
      "Model weights saved in ./qa/results/checkpoint-200/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/trainer.py:1332\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1330\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1332\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1335\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1337\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1338\u001b[0m ):\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/trainer.py:1891\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 1891\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1894\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/trainer.py:1923\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1923\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1839\u001b[0m, in \u001b[0;36mBertForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1827\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1828\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1837\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1839\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1851\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1853\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    989\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    990\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    991\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    995\u001b[0m )\n\u001b[0;32m--> 996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1009\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    578\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    583\u001b[0m     )\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:513\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    510\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    511\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 513\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/modeling_utils.py:2370\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2367\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   2368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m-> 2370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:526\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    525\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 526\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:441\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    439\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    440\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 441\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlayer_norm(\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_shape, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1164\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_full_backward_hook\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_full_backward_hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1166\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e0dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bb1fa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./qa/results\n",
      "Configuration saved in ./qa/results/config.json\n",
      "Model weights saved in ./qa/results/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"question-answering\", './qa/results', tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b56c0676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.13000917434692383, 'start': 15, 'end': 25, 'answer': 'California'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Where is Sinan living these days?\", \"Sinan lives in California but Matt lives in Boston.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fad03889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.010304080322384834,\n",
       " 'start': 119,\n",
       " 'end': 138,\n",
       " 'answer': 'Sir George Carteret'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "princeton = \"\"\"In 1675, a Quaker missionary from England, encouraged by New Jersey proprietors John Lord \n",
    "              \"Berkeley and Sir George Carteret, arrived to establish a settlement in this area near the \n",
    "              \"Delaware River, which was inhabited by the Lenni-Lenape Indians. The Keith survey of 1685 \n",
    "              \"established the western boundary of Middlesex and Somerset Counties and later, the Township \n",
    "              \"of Princeton. Today Keith's Line is recognized as Province Line Road. With the laying of the \n",
    "              \"cornerstone for Nassau Hall in 1754, Princeton began its development as a location for \n",
    "              \"quality education. Nassau Hall was named for William III, Prince of Orange-Nassau. This simple stone \n",
    "              \"edifice was one of the largest public buildings in the colonies and became a model for many other \n",
    "              \"structures in New Jersey and Pennsylvania.\"\"\"\n",
    "\n",
    "pipe(\"Who founded Princeton?\", princeton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c484d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bee15f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2073, 2003, 8254, 2319, 2542, 2122, 2420, 1029,  102, 8254, 2319,\n",
       "         3268, 1999, 2662, 2021, 4717, 3268, 1999, 3731, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_input = bert_tokenizer(\n",
    "    \"Where is Sinan living these days?\", \"Sinan lives in California but Matt lives in Boston.\",\n",
    "    return_tensors='pt'\n",
    ")\n",
    "qa_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efa803db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-1.2778e-03,  5.1027e-01, -2.5812e+00,  4.0053e+00,  1.4273e+00,\n",
       "          1.9147e+00,  2.1659e-01, -1.6363e-01, -1.8866e+00, -6.9708e-01,\n",
       "          3.4769e+00,  2.2316e+00,  1.8427e+00,  2.0021e+00,  5.3063e+00,\n",
       "          5.4442e-01,  4.6581e+00,  4.1498e+00,  6.3127e-01,  4.0223e+00,\n",
       "          5.3790e-01,  5.8757e-01]], grad_fn=<CloneBackward0>), end_logits=tensor([[-1.1980, -0.4949, -4.7951,  1.4667,  2.4398,  0.0896, -2.2514, -0.8370,\n",
       "         -3.3522, -2.8680,  0.5692,  2.6074, -0.8025, -1.0503,  5.3497, -1.1677,\n",
       "          2.4222,  2.4903, -2.6778,  4.1600, -0.4561, -0.9943]],\n",
       "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = qa_bert(**qa_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71fa3bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'where', 'is', 'sin', '##an']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_labels = bert_tokenizer.convert_ids_to_tokens(qa_input['input_ids'].squeeze())\n",
    "token_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddf6c4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIAAAAF0CAYAAABbk0LyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABPyUlEQVR4nO3dd1iV9eP/8ddhSY5ygWZahrvMnYqZZilOEHKVSq78iDnSUtE0NcWc5aDh+DS1j5kDV24tc7ejTM1MM8OBA9IAGef+/eGP85UcASI3983zcV1dF5yDnNfpcJ9z36/7fb/fDsMwDAEAAAAAAMC23MwOAAAAAAAAgNuLAggAAAAAAMDmKIAAAAAAAABsjgIIAAAAAADA5iiAAAAAAAAAbI4CCAAAAAAAwOYogAAAQK6oUqWKAgMD1b59+wz/nThxIku/p1+/flqxYkWG286cOaMHHnhAcXFxrttee+01ValSRb/99pvrtgULFmjIkCG38jRUu3bt62a+dOmSxowZo8DAQAUFBSk4OFhLly69pccCAADIKR5mBwAAAPnHBx98oOLFi+f47/X19VXlypX19ddfq3nz5pKkzz//XM2aNdO2bdvk5+cnSdq7d68CAwNz/PGlK4VTwYIFtXr1ajkcDp0+fVpdunTR3XffrcaNG9+WxwQAAMgsCiAAAGC6ffv2aebMmSpXrpwOHz6s1NRUvfLKK6pbt65Onz6tkSNH6syZMypTpozOnTt33d/RpEkT7du3T82bN9eJEyeUnJysHj16KDIyUs8++6ySk5P13Xffadq0aUpJSdGUKVO0Z88eubu7q0aNGho1apQKFy6sxx9/XDVq1NChQ4f0wgsvqFixYpo4caIcDoceeughOZ3O6z5+bGysSpQooZSUFHl5ealUqVKKjIxU0aJFJUlHjx7V2LFjdf78ebm5ual///5q06aNDh8+rAkTJiguLk4Oh0O9e/dWcHCw9u3bp0mTJqlgwYL6+++/tXz5cu3cuVNvv/22UlJS5O3trfDwcNWuXVtHjhzR6NGjlZycLMMw1LFjR3Xr1u12vVwAAMCCKIAAAECu6dGjh9zc/u8K9LJly+rNN9+UJEVHR2vcuHGqVq2a3n33Xc2cOVOLFi3ShAkTVLNmTQ0ZMkS///67goODr/u7mzRpookTJ0qSPvvsMz322GOqV6+efv31V50/f15HjhxRxYoVVaJECc2ZM0dnzpzRqlWr5O7urtGjR2vatGmaMGGCJKlSpUqaNWuWkpOT1axZM82YMUP+/v5au3atPvnkk+s+/sCBA/X888+rYcOGql27turUqaM2bdqoXLlykqQXXnjBVcycPHlSoaGhatKkifr3768RI0YoICBAp0+fVqdOnXTfffdJkg4fPqwtW7bonnvu0bFjxzRz5kx9+OGHKlasmA4fPqxevXpp06ZNeuedd/T444/rP//5j2JjY/Xqq6/q6aefzvD/GgAA5G8UQAAAINfc7BKwMmXKqFq1apKkBx54QFFRUZKk3bt3Kzw8XJJ03333qUGDBtf997Vq1dLJkycVFxenzz77TH379pWnp6caNmyovXv36siRI2ratKkk6YsvvtDQoUPl6ekpSQoNDdWAAQNcv6tevXqSpF9++UUeHh7y9/eXJLVr105jx4697uNXrVpVGzZs0P79+/XVV19p165dmjt3rmbPnq06dero4MGD6tSpkyTp7rvv1pYtW/Trr7/q8uXLCggIkCSVKlVKAQEB2rFjhxo0aKC7775b99xzjyRp165dOnPmjHr27Ol6TIfDoePHj6tFixYKDw9XdHS0/P39NWbMGMofAACQAXsGAAAgT/D29nZ97XA4ZBjGNV9LkofH9c9feXh4qGHDhvriiy904MABV4nTtGlTffPNN9q7d68ee+wxSZLT6ZTD4XD9W6fTqZSUFNf3BQsWdH199WPf6PFTU1M1duxYxcfHq3r16urVq5f++9//qn///lqyZInr31z9mL/99pvS0tIy3Jb+eKmpqdfkcDqd8vf316pVq1z/ffLJJ6pUqZKaNWumjRs3qnXr1jpw4IACAwN16tSp6/5/AgAA+RMFEAAAyNMeffRRLVmyRJIUExOjffv23fBnmzRpov/+97+qX7++a3RP06ZNtWfPHp08eVIPPPCA63cuXrxYKSkpcjqd+uijj/TII49c8/uqVKkiwzC0fft2SdLWrVsVHx9/zc95eHjo6NGjeuutt1xFUmpqqo4cOaIHHnhAhQsX1oMPPqiVK1dKkk6ePKmnn35ad955pzw8PLRp0yZJ0unTp7Vx40Y1atTomsfw9/fXrl27dOTIEUnS9u3bFRQUpKSkJL344otat26d2rZtq3Hjxqlw4cI6fvx4pv7/AgCA/IFLwAAAQK755xxA0pW5ca4e/fNP48aN06hRo9S6dWuVLl1aVatWveHPNmnSRKNHj1bv3r1dt5UsWVIFCxZUrVq1XKNt+vfvr6lTpyo4OFipqamqUaOGXn755Wt+n6enp958802NHz9er7/+uqpVq6YSJUpc97Fnz56t6dOnq2XLlrrjjjvkdDrVokUL16Vlr732ml555RUtXLhQDodDkyZN0t1336233npLERERioyMVFpamgYMGKCGDRteU3RVrFhREyZM0AsvvCDDMOTh4aG3335bhQoV0nPPPafRo0dryZIlcnd3V/PmzfXwww/f8P8TAADIfxzGP8c1AwAAAAAAwFa4BAwAAAAAAMDmKIAAAAAAAABsjgIIAAAAAADA5iiAAAAAAAAAbI4CCAAAAAAAwOYogAAAAAAAAGzOw6wHvnDhbzmdrEAPAAAAAABwq9zcHCpWrNAN7zetAHI6DQogAAAAAACAXMAlYAAAAAAAADZHAQQAAAAAAGBzFEAAAAAAAAA2RwEEAAAAAABgcxRAAAAAAAAANkcBBAAAAAAAYHMUQAAAAAAAADZHAQQAAAAAAGBzFEAAAAAAAAA252F2AAAAAAD/p0jRgvL2dDc7xk0lpaTpYlyC2TEAAFlAAQQAAADkId6e7uq0/CezY9zU0g7VddHsEACALOESMAAAAAAAAJujAAIAAAAAALA5CiAAAAAAAACbowACAAAAAACwOQogAAAAAAAAm6MAAgAAAAAAsDkKIAAAAAAAAJujAAIAAAAAALA5CiAAAAAAAACbowACAAAAAACwOQogAAAAAAAAm6MAAgAAAAAAsDkKIAAAAAAAAJujAAIAAAAAALA5CiAAAAAAAACbowACAAAAAACwOQogAAAAAAAAm6MAAgAAAAAAsDkKIAAAAAAAAJujAAIAAAAAALA5CiAAAAAAAACbowACAAAAAACwOQogAAAAAAAAm6MAAgAAAAAAsDkKIAAAAAAAAJujAAIAAAAAALA5CiAAAAAAAACb88iJXzJ16lRduHBBU6ZMyYlfBwAAAABAvlH8roJy93I3O8ZNpSWn6Xx8gtkxcAtuuQDas2ePoqKi9Nhjj+VAHAAAAAAA8hd3L3edmvGb2TFuqvQwP7Mj4Bbd0iVgcXFxmjlzpsLCwnIqDwAAAAAAAHLYLRVAY8eO1dChQ3XnnXfmVB4AAAAAAADksGxfArZ06VLdfffd8vf314oVK7L870uUKJzdhwYAAABgMh+fImZHAJDL2O6tLdsF0Lp16xQbG6v27dsrPj5eCQkJevXVV/XSSy9l6t+fO3dJTqeR3YcHAAAAbMkqB1ixsRfNjgDYBts9coKbm+Omg22yXQC99957rq9XrFihL7/8MtPlDwAAAAAAAHLPLc0BBAAAAAAAgLzvlpeBl6Qnn3xSTz75ZE78KgAAAAAAAOQwRgABAAAAAADYHAUQAAAAAACAzVEAAQAAAAAA2BwFEAAAAAAAgM1RAAEAAAAAANgcBRAAAAAAAIDNUQABAAAAAADYHAUQAAAAAACAzVEAAQAAAAAA2BwFEAAAAAAAgM1RAAEAAAAAANgcBRAAAAAAAIDNUQABAAAAAADYHAUQAAAAAACAzVEAAQAAAAAA2BwFEAAAAAAAgM1RAAEAAAAAANgcBRAAAAAAAIDNUQABAAAAAADYHAUQAAAAAACAzVEAAQAAAAAA2BwFEAAAAAAAgM1RAAEAAAAAANgcBRAAAAAAAIDNUQABAAAAAADYHAUQAAAAAACAzVEAAQAAAAAA2BwFEAAAAAAAgM1RAAEAAAAAANich9kBAAAArKZIUW95e3qaHeOmklJSdDEuyewYAAAgj6AAAgAAyCJvT0+1W/aR2TFuam3HbrooCiAAAHAFl4ABAAAAAADYHCOAAAAAACAfKVa0kDw88+5YgNQUpy7E/W12DMB2KIAAiyl6l5c8vQqYHeOmUpIvKy4+2ewYAAAAOaZo0ULyzMOlSUqKU3GZLE08PN30xcLY25wo+5qE+pgdAbAlCiDAYjy9Cmjtu63NjnFT7Xqvl0QBBAAA7MPT002fLD9rdowb6tyhpNkRAORxebfCBgAAAAAAQI6gAAIAAAAAALA5CiAAAAAAAACbowACAAAAAACwOQogAAAAAAAAm7ulVcDeeOMNrV+/XpLUtGlTjRgxIkdCAQAAAAAAIOdkewTQ7t27tXPnTkVFRWnlypXav3+/Nm/enJPZAAAAAAAAkAOyPQLIx8dHI0eOlJeXlySpQoUKiomJybFgAAAAAAAAyBnZLoAqVark+vrYsWNav369Fi9enCOhAAAAAAAAkHNuaQ4gSTp8+LD69eunESNGqHz58pn+dyVKFL7VhwaQh/n4FDE7AgDke7wX43bi7yvvsdNrYqfnYie8LtZ2SwXQN998o8GDB+ull15S27Zts/Rvz527JKfTuJWHB/Ilq7zpxsZeNDsCANw2vBfjduLvK2+ywuuS2dfETs/FLqzwmkj573WxGjc3x00H22S7ADp58qQGDBigmTNnyt/fP7u/BgAAAAAAALdZtgugd955R5cvX9aUKVNctz311FN6+umncyQYAAAAAAAAcka2C6AxY8ZozJgxOZkFAAAAAAAAt4Gb2QEAAAAAAABwe1EAAQAAAAAA2BwFEAAAAAAAgM1RAAEAAAAAANgcBRAAAAAAAIDNUQABAAAAAADYHAUQAAAAAACAzVEAAQAAAAAA2JyH2QGA3FDsLi95eBUwO8ZNpSZf1oX4ZLNjAAAAAABsiAII+YKHVwHtnt/O7Bg31eg/ayVRAAEAAAAAch6XgAEAAAAAANgcBRAAAAAAAIDNUQABAAAAAADYHAUQAAAAAACAzVEAAQAAAAAA2BwFEAAAAAAAgM1RAAEAAAAAANgcBRAAAAAAAIDNUQABAAAAAADYHAUQAAAAAACAzVEAAQAAAAAA2BwFEAAAAAAAgM1RAAEAAAAAANgcBRAAAAAAAIDNUQABAAAAAADYHAUQAAAAAACAzVEAAQAAAAAA2BwFEAAAAAAAgM1RAAEAAAAAANgcBRAAAAAAAIDNeZgdAAAA5B9FinrL29PT7Bg3lJSSootxSWbHAAAAyHEUQAAAINd4e3qq7fJ5Zse4oU879NNFUQABAAD74RIwAAAAAAAAm6MAAgAAAAAAsDkKIAAAAAAAAJujAAIAAAAAALA5CiAAAAAAAACbowACAAAAAACwOQogAAAAAAAAm6MAAgAAAAAAsLlbKoDWrFmjNm3aKCAgQB999FFOZQIAAAAAAEAO8sjuPzx9+rRmzpypFStWyMvLS0899ZQaNGigihUr5mQ+ADZWtKinPD29zY5xQykpSYqLSzE7BgAAAADcsmwXQLt371bDhg1VtGhRSVLLli21YcMGDRw4MKeyAbA5T09vffB+gNkxbqhHz02SKIAAAAAAWJ/DMAwjO/9w3rx5SkhI0NChQyVJS5cuVXR0tCZOnJitIEZqmhwe7tn6t7klsxmN1FQ5PLLdreWKzGZ0pibLzcMrFxJlT2bz5fXnIWU+Y1pqstzz+HPJbMa8/lyyki81LVke7nn3uWQ2X0pasjzz8POQMp8xOS1FXu6euZAo+zKbMTktVV7uefdzJSv57PJcktPS5OWet/dbMpuR55J7Mv88nPJyz9tTdWY2Y2qaIQ93Ry4kyr7MZkxLM+Seh59LVvI50wy55eHnktl8zlRDbh5593lImc9opDrl8Mjb231mM+b155KVfHm9p8hqvmzvgTmdTjkc//eHbBhGhu//zblzl+R0/l/35ONTRLFvL8punFzh07+7YmMv/vvP+RTR6ben5EKi7CvVf2Smn8uRyPa5kCh7KgxalannccXl25olZ2Q2I88l92Qun49PEc38X8vbnCX7hnbdmOlt/oXlrXIhUfa93mFDpp9L61VhuZAo+9a3n5vp59ImKu9+rqwLydxnCvImH58iClq2yuwYN7W6Y/tMbyshyz+//YGyKarDY/luW/HxKaKJUTFmx7ipl0PK5LvXxS58fIro4FunzY5xU1WfK5Xv/r58fIro9KwvzY5xQ6WG1M/0a+LjU0Rn3thwmxNln+/AVhmei5ubQyVKFL7hz2e7litdurRiY2Nd38fGxsrX1ze7vw4AAAAAAAC3SbYLoEaNGmnPnj06f/68EhMTtWnTJjVp0iQnswEAAAAAACAHZPsSsFKlSmno0KF65plnlJKSoo4dO6pGjRo5mQ0AAAAAAAA54JZmYQwMDFRgYGBOZQEAAAAAAMBtkHen5gYAAAAAAECOoAACAAAAAACwOQogAAAAAAAAm6MAAgAAAAAAsDkKIAAAAAAAAJujAAIAAAAAALC5W1oGHteXlpyiUv1Hmh3jptKSU8yOAAAAAACwmbTkNJUaUt/sGDeUlpxmdgTTUADdBufjkyQlmR0DAAAAAIBcdT4+wewIuAEuAQMAAAAAALA5CiAAAAAAAACbowACAAAAAACwOQogAAAAAAAAm6MAAgAAAAAAsDkKIAAAAAAAAJtjGXgAAIB8LCklVas7tjc7xk0lpaSaHQEAAMujAAIAuFxOuazXO2wwO8ZNXU65bHYEwFYuxiXqotkhAADAbUcBBABw+SsuWVKy2TEAAAAA5DDmAAIAAAAAALA5CiAAAAAAAACb4xIwAAAAAACAf0hLTpXvwFZmx7ihtOSsLZJAAQQAAABbSEpJVVSHx8yOcUOsZgYA1nI+PtHsCDmKAggAAAC2wIpmAADcGHMAAQAAAAAA2BwjgAAAAADcFskpaXo5pIzZMW4qOSXN7AgAkCvyTAGUlpwin/7dzY5xU2nJKWZHAAAAACwjPi7B7AgAgP8vzxRA5+OTJCWZHQMAAAAAAMB2mAMIAAAAAADA5iiAAAAAAAAAbI4CCAAAAAAAwOYogAAAAAAAAGyOAggAAAAAAMDmKIAAAAAAAABsjgIIAAAAAADA5iiAAAAAAAAAbI4CCAAAAAAAwOYogAAAAAAAAGyOAggAAAAAAMDmKIAAAAAAAABsjgIIAAAAAADA5iiAAAAAAAAAbC7bBdA333yjjh07qn379urRo4f+/PPPnMwFAAAAAACAHJLtAmj48OGKiIjQqlWrFBgYqIiIiJzMBQAAAAAAgBySrQIoOTlZzz//vKpWrSpJqlKlik6ePJmjwQAAAAAAAJAzPLLzj7y8vNS+fXtJktPp1BtvvKHmzZtn6XeUKFE4Ow8NXMPHp4jZEQBLYFvJm+zyutjleQAAkNP4jERe8a8F0Pr16zV58uQMt/n5+en9999XcnKyRo4cqdTUVPXr1y9LD3zu3CU5nUbW0iLXWeHNKjb2otkRALaVPMgKr4mUudfFCs8lv/19AQDMZ4XPR4nPSOQeNzfHTQfb/GsB1Lp1a7Vu3fqa2//++2/1799fRYsW1dtvvy1PT89bSwoAAAAAAIDb4pYmgb7vvvs0a9YseXl55WQmAAAAAAAA5KBszQH0888/a+vWrapYsaJCQkIkSb6+vlqwYEGOhgMAAAAAAMCty1YB9MADD+jQoUM5nQUAAAAAAAC3QbYKIABARskpSRradaPZMW4oOSXJ7AgAAAAATEQBBAA5ID4uRVKK2TEAAAAA4LqyPQk0AAAAAAAArIECCAAAAAAAwOYogAAAAAAAAGyOAggAAAAAAMDmKIAAAAAAAABsjgIIAAAAAADA5iiAAAAAAAAAbI4CCAAAAAAAwOYogAAAAAAAAGzOw+wAAAAAAABkVWqyU1WfK2V2jJtKTXaaHQFwoQACAAAAAFjOhfi/zY4AWAqXgAEAAAAAANgcBRAAAAAAAIDNcQkYAAB5XFJKitaFjDQ7xg0lpaSYHQEAAAD/ggIIAIA87mJcki4qyewYAAAAsDAuAQMAAAAAALA5CiAAAAAAAACbowACAAAAAACwOQogAAAAAAAAm6MAAgAAAAAAsDkKIAAAAAAAAJujAAIAAAAAALA5CiAAAAAAAACbowACAAAAAACwOQogAAAAAAAAm/MwOwAAALdDUkqy1refa3aMm0pKSTY7AgAAAPIJCiAAgC1djLusi7psdgwAAAAgT+ASMAAAAAAAAJujAAIAAAAAALA5CiAAAAAAAACbowACAAAAAACwOQogAAAAAAAAm6MAAgAAAAAAsDkKIAAAAAAAAJujAAIAAAAAALA5CiAAAAAAAACbowACAAAAAACwuVsugH7++WdVr149J7IAAAAAAADgNrilAigxMVETJ05USkpKTuUBAAAAAABADrulAmjKlCnq0aNHTmUBAAAAAADAbZDtAmjr1q1KSkpSq1atcjIPAAAAAAAAcpjHv/3A+vXrNXny5Ay3+fn56dKlS3r//fez/cAlShTO9r8FrubjU8TsCAAAAAAA5GkOwzCMrP6jpUuXat68eSpUqJAk6eDBg6patao++ugjFS6cuWLn3LlLcjqz/NDIZT4+RXQksr3ZMW6owqBVio29aHYMAAAAAABM5ebmuOlgm38dAXQ9nTp1UqdOnVzfV6lSRatWrcrOrwIAAAAAAMBtdsvLwAMAAAAAACBvy5EC6NChQznxawAAAAAAAHAbMAIIAAAAAADA5iiAAAAAAAAAbI4CCAAAAAAAwOYogAAAAAAAAGyOAggAAAAAAMDmKIAAAAAAAABsjgIIAAAAAADA5iiAAAAAAAAAbI4CCAAAAAAAwOYogAAAAAAAAGyOAggAAAAAAMDmKIAAAAAAAABsjgIIAAAAAADA5iiAAAAAAAAAbM5hGIZhxgOfO3dJTqcpD40sKHaXlzy8Cpgd44ZSky/rQnyy2TEAAAAAADCVm5tDJUoUvuH9HrmYBRZ0pVyhYAEAAAAAwMq4BAwAAAAAAMDmKIAAAAAAAABsjgIIAAAAAADA5iiAAAAAAAAAbI4CCAAAAAAAwOYogAAAAAAAAGyOAggAAAAAAMDmKIAAAAAAAABsjgIIAAAAAADA5iiAAAAAAAAAbM7DrAd2c3OY9dAAAAAAAAC28m89i8MwDCOXsgAAAAAAAMAEXAIGAAAAAABgcxRAAAAAAAAANkcBBAAAAAAAYHMUQAAAAAAAADZHAQQAAAAAAGBzFEAAAAAAAAA2RwEEAAAAAABgcxRAAAAAAAAANkcBBAAAAAAAYHMUQAAAAAAAADbnYXaAnBATE5OpnytTpsxtTgIAAAAAAJD3OAzDMMwOcauqV6+uUqVK6WZP5ezZs4qOjs7FVEgXFxenn3/+WY0aNdK8efO0f/9+DRs2TPfee6/Z0bLsr7/+0po1axQXF5fh723gwIEmpsqeFStWaOrUqfrrr78kSYZhyOFw6MCBAyYny5rQ0FA5HA7X9w6HQ97e3vLz81NYWJjuuusuE9Nl3Zo1a/Trr78qLCxMGzduVHBwsNmRsiw6OlrffPONunXrprCwMP3888+aNm2amjRpYna0LDl9+rQmTpyo06dPq1OnTurcubPZkbLlmWee+defcTgc+uCDD3IhDW7k8OHDio+Pz/DZ8vDDD5uYCHZ9TQzD0IkTJ1SuXDmzo2TZvHnz1K9fvwy3vf7663rhhRdMSpQ9EydO1Msvv5zhtvDwcE2dOtWkREhn1+3eLpYsWaIuXbqYHSNTnnjiiZvebxiG3NzctGXLllxKdIUtRgBVrFhRK1euvOnPWPEgKj4+XtOnT9fx48c1Z84cTZ06VSNHjrTcAe2LL76oRo0aSZI2bNigHj16aPTo0Vq4cKHJybLu+eefV5EiRVSpUqUMpYMVvfXWW1q4cKEqV65sdpRbUrFiRXl4eKhDhw6SpLVr1+rUqVMqVaqURo8erTfeeMPkhJk3Y8YMnTp1Svv371ffvn21fPlyHTx4UCNHjjQ7WpZERERo8ODB2rhxo7y9vRUVFaWBAwdargAaM2aMGjVqpPr162vSpEk6e/asnnvuObNjZdmpU6cUERFxw/sNw7jmQMQK/vzzTy1atOiaHfXJkyebmCp7XnnlFX322WcZDsgdDoc+/PBDE1NlnV3KX8k+r4kkffzxx5o2bZoSExNdt91zzz25ftBxK2bMmKFz585p27ZtOnbsmOv21NRURUdHW6YAGj16tP744w/99NNPOnz4sOv2tLQ01wk5q2G7z3t+/vlnzZ0795rPR6s9j+s5ffq02REyzdvbW/Pnz7/h/YZhXFNo5wZbFEBLliy55rYLFy6oaNGiroP06/1MXvfyyy/rkUceUXR0tAoWLChfX18NHz78pn9IeVF8fLz69OmjiRMnKiQkRMHBwZZ9Azp79qzee+89s2PkCF9fX8uXP5L0ww8/aMWKFa7vq1atqg4dOmjGjBn/WgznNTt37lRUVJRCQkJUuHBhvffeewoKCrJcAeR0OtW4cWO9+OKLCggI0N133620tDSzY2VZbGysevXqJUlasGCBBg8erFatWmnSpEmaP3++5s2bZ3LCzBkyZIjq16//rz9jNUOGDFG9evVUr149yxfyu3bt0oYNG+Tt7W12lFtil/JXss9rIknz58/XqlWrNGvWLA0dOlTbt2/Xt99+a3asLAkICNCRI0e0d+/eDO9n7u7uGjBggInJsqZ///76888/NWnSpAyjx93d3VWhQgUTk2Uf233eEx4eri5dutjihPU/DR48WJI1RgK98soruueee/71Z3KbLQqgv//+W8OHD1e3bt308MMPa9CgQdq1a5dKliypuXPnqmLFiipQoIDZMbPsxIkT6tKlixYvXiwvLy8NHTpUQUFBZsfKMqfTqZ9++klbtmzRokWLdODAAUseDEpStWrVdPDgQVWtWtXsKLfswQcf1ODBg/XII49k2D6sNlouJSVFhw8fVqVKlSRdGbrrdDqVlJSklJQUk9NljZvblXn50z+sk5OTXbdZyR133KF3331X+/bt09ixY/Xhhx+qUKFCZsfKMofDod9++01+fn4qVKiQ3nnnHZ0+fVqFChXSoEGDzI6XaXXr1tWLL76ow4cPq1atWho2bJjuvPPODD/Tpk0bk9JlX2pqqsLDw82OkSPKlSt308vYrcIu5a9kn9dEkkqUKKFy5cqpSpUq+uWXX9StWzctXrzY7FhZUqNGDdWoUUMtWrRQ4cKFzY6TbWXLllXZsmXVtm3ba4p5K17KJrHd50Xe3t7q3r272TFuq48//jjPF0B33nmnnnrqKdf+18SJE6+Zk7hevXq5nssWBdDEiRNVvXp1Va9eXRs2bNCBAwe0c+dOHT58WJMmTbLsiA13d3ddvHjRdTB47NgxSx4MDh8+XNOmTVOvXr1Urlw5de7c2XIjGtIdPnxYISEhKlGihAoUKOCaN2fr1q1mR8uyS5cuqVChQvr+++8z3G61AmjMmDHq27evSpQoIafTqb/++kvTpk1TZGSk2rdvb3a8LGnVqpWGDBmi+Ph4vf/++1q1apXatWtndqwsmzFjhpYuXao5c+borrvu0unTp/Xaa6+ZHSvLhg0bpu7du2vMmDGugqRUqVKSrsw9ZxUvvfSSKleurMDAQG3cuFGTJ0+25GVS/1S3bl1t27ZNjRs3lpeXl9lxbsldd92ltm3bqnbt2hmei9Vep/Tyd+/evZYufyX7vCbSlddl7969qlKlirZs2aKHHnpISUlJZsfKluuN+PPx8dEXX3xhUqKsudGlbGlpafrhhx8sWQCx3ec9jRs31sKFC9W4ceMMJ3nttCCSFYq68ePHq127dmrQoIHWrl2rKVOmaM6cOWbHssck0IGBgVqzZo0kaeTIkSpevLhGjBghSWrbtq0+/fRTM+Nl244dO/Taa6/p5MmTqlu3rr7//nu9+uqreuyxx8yOlm/9+eef173934b3WUVSUpIlh72mpqbql19+kZubmypUqCBPT09XOWc1O3bs0O7du+V0OuXv72/J7b1v37568skn9cQTT1j+wPzy5ctKSkq64dxrn332mZo1a5bLqbKmXbt2Wrt2raQrI+aCg4Mt+7l4tcaNG+vs2bMZbrPiRPaSFBUVdd3bQ0JCcjnJrTl9+rSWLl2qRx55RLVr19b06dMVGhqq0qVLmx0ty+zymkjSL7/8omXLlmnkyJF6/vnntXv3bg0aNEg9e/Y0O9otSUlJ0ZYtW/T9999r1KhRZsfJlOjoaB05ckRz5sxxXcoiXTnpW6NGDZUvX968cNnEdp/3PP7449fcZtUT1jcSEhJyw9crrwgKCtLq1atd3+eVXsIWI4CuPsjbu3dvhskur57wzmp8fHz07rvvKjo6WmlpaZowYYJKlixpdqxMS98wq1atet0DcSvtpKcf5H311VfXvd+KBdC2bds0a9YsJSQkyDAM12VTe/bsMTtalthpItjk5GT5+PgoPDxcq1ev1r59+1SjRg0VL17c7GhZ0rdvX61cuVLTp09X06ZN9eSTT+qhhx4yO1a2FChQ4KaXEM+ZMyfPF0Cenp4Zvr76eyvbuXOn2RFyTEhIiOLi4pSYmCjDMJSWlqYTJ06YHSvLSpUqpYYNG+rgwYN68MEH9dhjj1nuIDA2NlY+Pj5q0KCB2VFyTOXKlfXSSy9JkiIjI01Ok3M8PT3VunVrzZ071+womZZ+KVvz5s1VpEgR1+3pK7NZEdt93rNt2zazI0CSh0fGqiWv7H/ZogAqU6aM1q1bp8TERCUmJrquqV21apVrXhArGjp0qNavX2/JEQDS/7XoBw8evOHPWOHsuST9+OOPatasmfbt23fd+6122ZR0pSCZOHGi3nvvPYWFhWnLli2WLEztNBHs8OHDVbZsWSUnJ+vNN99UUFCQRo0aZZnJhtPVr19f9evXV1JSkjZs2KCBAweqSJEi6tixo7p27Wr5UUFXs+IgWqtvJ+kTP95ohb+rJ1a1isjISL3//vtKTU1VsWLFdPr0aVWvXl1Lly41O1qWfPDBB9qyZYvOnDmjVq1aaezYserYsaP69OljdrRMGzNmjObNm6fu3bvL4XC4RpNa8ZLvfv36ad68eXr88cevu91b6bmku3pxB8MwdPjw4WsOsqxg3bp1mjp1qqVXZkvHdp/3nD9/XhMmTNCePXuUlpamhg0bavz48ZYaSGAH/9xHzCv7X9Z7x7yOcePGaezYsTp37pxee+01eXl5afLkyfrss88st2LW1SpWrKg33nhDNWvWzHBZzsMPP2xiqpxlhbPn0v/NOH/1qJJLly7p5MmTli0ZixQpooYNG+rbb7/VxYsXNXz4cCaCNdmJEyc0e/ZsTZ8+XR06dNB//vMf1/L2VrNv3z6tWrVKu3btUpMmTdSmTRvt3r1b/fv31zvvvGN2vByTVz7Mb+bw4cN64oknXN+fPn1aTzzxhGV3bK1Yuv2bqKgobd++XZMmTVL//v3122+/6X//+5/ZsbIsKipKn3zyiTp37qxixYpp2bJl6tSpk6UOBNML95udQbfC6jPSlTkyJWnhwoUmJ8k5/zwRV6xYMc2aNcucMLdg3rx5ll+ZLR3bfd4zduxY1a5dWxEREXI6nVqyZIlGjx5tuROKN3P1CLq86sCBA6pWrZrre8MwVK1aNdf+l1lXw9iiALr77ru1YMGCDLc999xzCg8Pt+Skyeni4uK0b9++DB92DofDskuoX4/VduSXLl2qb775RiNGjFBwcLAKFSqk9u3bKywszOxoWebt7a2jR4+qQoUK+vLLL9WwYUPLrZol2Wsi2LS0NJ0/f15btmxRZGSkYmNjdfnyZbNjZVmzZs1UtmxZdejQQWPHjnUV2A0aNLBsoWVlGzduNDtCjnrqqackXRnpk5ycLC8vL/3+++86evSoJZcdliRfX18VLlxYlSpV0sGDBxUQEGDJidPd3NwyvA8XKFBA7u7uJia6Payw+ox05e9KunKJzs6dOxUXF5fhfitevp5+Iu7SpUvy8PCw5LyFkj1WZkvHdp/3/PHHHxlGyfbt2zfDXDRWsGPHDm3YsEGnTp2Sm5ubfH191aRJE7Vs2VKSLHE8fLOrYMxkiwLoetIn7KxTp45lG3U7nbG5ESucPb/a4sWLNXfuXK1du1ZPPPGERo8erc6dO1uyABo6dKhmzZql6dOna8GCBVqyZIk6duxodqws27BhgxYtWpThNqtOBNunTx917txZjz/+uCpXrqyWLVvq+eefNztWln3wwQe69957r7ndzc0tz0/YZ0fnzp1TjRo1rnvfqlWrLHkQKElvvvmmjhw5omHDhqlbt26qVKmSdu3apdGjR5sdLcsKFy6slStX6sEHH9SiRYvk6+tryVWa6tev77qsZcuWLVqyZIkaNmxodqwcZ7WTVy+++KJiYmJUoUKFDPtdVrx8/ZdfflF4eLhiYmIkSX5+fpo6dep1P3PyMjutzMZ2n/c4HA6dPHlSd999tyQpJibGUpdKzp49W9HR0QoKCpKvr68Mw1BsbKyWLVum77//3jIj/7ds2aLmzZtLkuLj4zMsKLJgwQL17dvXnGCGzdWqVcvsCNl24sQJo2fPnkaLFi2MM2fOGKGhocYff/xhdqwcFRwcbHaELAkJCTEMwzB69+5tfP7554ZhGEabNm3MjJRt27dvd339+eefG3FxcSamwdXSX4uUlBSTk2TPd999Z4SFhRnPPPOMERoaanTr1s1o1qyZ2bGyZffu3ca3335rGIZhvPPOO0a/fv2MyMhI4/Lly4ZhGEb79u1NTJc5V7/Pdu7c+Yb3WU1ISIiRmJhozJs3z5g6darrNis6deqU8c477xiGYRhTpkwxAgMDjbVr15qcKuvS0tKMxYsXG4MGDTIGDBhgLFy40LLvYzdjte2mZcuWZkfIMV26dHHtfxmGYWzatMno1q2biYmy55dffjEmTZpkpKWlGQMHDjTq1KljvPfee2bHyha2+7xn27ZtxqOPPmoMHDjQGDBggNG4cWPjs88+MztWpgUEBBhpaWnX3J6ammq0atXKhETZc/XfzD//fsz8e7JOFZhNVhthcrWxY8eqT58+mjFjhkqWLKl27dopPDxcH330kdnR8q2KFSuqX79+OnHihPz9/TVkyJAbnlnPq0JDQ1WrVi1t2rRJ999/v8qVK6dZs2ZZbmSGHSeCPXjwoIYMGaKkpCQtWbJE3bt316xZs/Tggw+aHS1LXnrpJfXp00dRUVEKDQ3Vpk2b9MADD5gdK8umTZumr7/+WqmpqSpbtqwcDoeefvppbdu2TRMmTFBERISWLFlidsx/ZVx11vKflxQaFjqj+U9Op1Pe3t767LPPNGTIEDmdTstNZH/58mUVKFBAy5cv13PPPSfpyrwGVhuqn27KlCkKCgpyXaaHvKFChQo6c+aM65IwK7t8+bKaNm3q+r5FixZ68803TUyUPZUqVbLNymxs93lPs2bNVKNGDf34449yOp165ZVXLDFnTroCBQro1KlTKlOmTIbbY2JiLDXdw9X7WP/c3zJz/8sWBVD6MNB/MgzD0ju3Fy5cUOPGjTVjxgw5HA517tzZcuXPli1bdPLkSTVt2jTD8Nz0g3ervT6vvvqqvvvuO1WqVEleXl4KCgpyzTlhlRXN5s6dq++++07Lli3T9OnTdeLECf3+++969dVXVatWLctMBG21v53MmDhxot588029+OKLKlWqlMaPH69x48Zp2bJlZkfLEi8vL3Xo0EF//vmn7rzzTk2bNk2BgYFmx8qyHTt2aNWqVUpOTtZjjz2mHTt2yNPTU02aNFH79u0l6aZLxOcVV58I+edJESufJPH391e7du3k7e2thx9+WN27d9fjjz9udqws6dGjh9zc3PTHH3/I19dXlStX1saNG11lkNXce++9mjRpkuLj4xUYGKjAwECVLVvW7Fj5XlJSklq1aqXKlStnOHiywhwa6dL39atWrar58+erY8eOcnd315o1a1SvXj2T02Xdhg0bNH/+fMXHx2e43WqT8kts93lRly5dtGTJEtdK0k6nU+3bt9eaNWvMDZZJI0eOVLdu3VS+fHn5+PjI4XDozJkzOnbsWIYFeawkL+1/2aIA6t69+w3vK1asWC4myVne3t46deqU6w/k66+/tlTrOWPGDP3000+qUKGC5s6dqxEjRrgOmtInUrPC2fOreXh4ZFiF7eqDDausaBYdHa3atWurdOnSmjNnjiQpKChIAQEB+u6770xOl3npZ5qKFCmidu3aqUSJEiYnunWJiYmqUKGC6/tHHnlEU6dONTFR9hQoUEBxcXG6//779cMPP8jf319paWlmx8oywzB08eJFJSQkKDExUZcuXVKxYsWUlJRkyQnT7SY8PFyhoaEqVaqU3Nzc9PLLL7tW27DKai0ff/yxEhMT1a5dO12+fFnLli3T8ePH1aVLF1WrVk3jx483O2KWdO/eXd27d9fJkye1bt06DRgwQIUKFbLkimY3Y6Uz6dKV+eWsNP/H9Vy9PPe+ffv08ccfu+5zOBwaM2aMiemyburUqZo2bdo1IxysiO0+73jmmWf05ZdfSrpSlqZvMx4eHpY6QdKoUSMNGzZMR48elbu7u8qWLavSpUurZs2aioqKsswcU3n1JJu1Pw3+v5st2Wdlo0aNUr9+/XT8+HG1b99e8fHxllrqcvv27YqKipKHh4dCQ0PVu3dveXl5qXXr1q7RG1Y4e55ZVhmRsmPHDr3xxhs6fvy4RowYoSpVqigpKUlVqlSx5Fm0U6dOqVOnTvLz81NQUJBatGihO+64w+xY2VK0aFEdPHjQ9YGxevXqDBPGWUXPnj01dOhQRUZGqlOnTlqzZo2qV69udqws69u3rwICAmQYhoYPH67evXvL399fe/bssdRqZjExMRo1atQ1X6d/b2VXHzxdvdSqVVZrmTVrlurWras777xT3bp1kyT9+OOPWrRokSUnspekixcvateuXdq1a5fS0tL0yCOPmB0py3bv3q0iRYqoWrVqioyM1KFDh1S3bl317t1b7u7ulho5I0nTp0+33GXe/5SZfX2rFL/SlVEzdevWtfRqxVez+nafmpqqlStXytvbWy1bttTkyZP11VdfqXr16goPD1fRokUtsd2nZ4yIiLBcKXq1GTNmaP/+/fLz89P69esVHh7uOgFvlc93STp27JieeeaZa742DEO///67abkchlWOWv/F8uXLValSJdd8LK+//rruu+8+S+2kX09KSoqOHTumtLQ0+fn5WWoEULt27bRq1SrXUpCHDx9Wr1699Nprr2nKlCmW3xn5p5CQEEs9p8DAQM2cOVOHDx9WRESEKlWqpEuXLlnucqN0X3/9tdatW6ddu3apZs2amjZtmtmRsuz48eMKDw/Xjz/+KG9vb913332aPn26/Pz8zI6WZYZhyOFwKCEhQceOHVO1atXy7JmQm0lKSlJaWpoKFSqkQ4cOaefOnapataqldm7/7X0pJCQkl5LknuDgYK1cudLsGP9q+/bt+vbbb/XBBx/Iz89PPj4+2r9/v1555RXVqlXLciMbw8LCtH//fgUEBCgoKEg1a9Y0O1KWTZ8+Xd9++60uXbokX19flShRQm3bttWGDRtUsGBBvfzyy2ZHzLK+ffuqX79+qlGjhqX2I7PKSvth27dv14IFC/Twww9nWDLdivMX2mG7HzlypBISEpScnKy4uDjVqFFDnTt31tatW7V//37XiHmruHDhgg4cOKBGjRpp3rx52r9/v4YPH65y5cqZHS1TAgMDXYMIjh07pt69e2v48OFq3bq1ZT7fJblGY91I/fr1cylJRrYYAbRw4UKtXr06w6USjRs31tSpU3X58mV17drVxHTZ98cff+jjjz/WhQsXMowuscq1j61atVJoaKhGjhypGjVqqFKlSpo9e7YGDhyo5ORks+Ple+3atVPFihVVsWJF/f777woLC5PT6TQ7VrYYhqGUlBSlpKTI4XDI09PT7EjZcu+992rx4sVKSEiQ0+lU4cKFzY6UJVePLLkeq7x3Xc3b29v1dZUqVVSlShUT02TP9QqeCxcuqGjRopYs5TLDKs+radOmatq0qbZv364VK1bo3Llzeuqpp3Tw4EEtWbJE8+fPNztilnTu3FlNmjSx9OVG27dv15o1axQXF6cWLVroyy+/lJubm5o0aWLJZdOlK6PK/jldgsPhsOwosxux0jntt99+W/fff3+G8seq7LDd79+/X2vWrFFaWpqaNm3qusSwYsWKrukrrGTYsGFq1KiRpCvzTfXo0UMvvfSSFi5caHKyzEk/iShJ5cuX17x589SrVy8VL17cMp/v0rUFj9Pp1M8//6x7771Xd955p0mpbFIALVu2TB999FGGg6X69etrwYIF6tmzp2ULoEGDBsnf31/16tWz1B97uoEDB6pu3boqVKiQ67a6detqxYoVevfdd01Mlr8NGzZM9erV09q1a9W7d295enpq48aNCgsLs+RQ5IiICG3evFnVqlVTUFCQxowZY9lLC6Ojo/Xuu+9eU/paYdix9H8fdJ999pn+/vtvBQUFycPDQ+vWrbPEtfN2df78eY0fP17dunXTww8/rMGDB2vnzp0qWbKk5s2bl2HeKZijXbt2kqQSJUqoc+fO6tu3r8mJsiYyMlKDBg3S5s2btXnz5mvut1r5m5ycrGLFiik8PNz1ufj3338rNTXV5GTZs3fvXrMj5Aor7SunpKRYbrv4Jztt925ubjp69KguXryoixcv6sSJEypbtqzOnz9vye0+Pj5effr00cSJExUSEqLg4GDL7EtK9hlE8Pvvv2vo0KEaPHiwGjVqpG7duuncuXNyOp167bXXVLduXVNy2aIAcnNzu+6Z8uLFi1vygDadYRgKDw83O8Yt8ff3v+a2u+++W6NHjzYhza05duyY7rjjDpUqVUpLly7VoUOHVKdOHdeqWVY58xQWFqZvv/1WJ0+eVGhoqFJSUhQTE6MPPvhAtWvXttyy9vfdd5+ioqJUvHhxs6PcsvDwcHXv3l0VK1a01I5suvSRJv/73/+0ZMkS1/tv69at1blzZzOj5WsTJ05U9erVVb16dW3YsEE///yzdu7c6br887333jM7Yr4VEhKi+++/X99//72qV6+uypUra926dZYrgB588EFJ5g1nz0ldu3ZVUFCQ1q1bp06dOkmSvv32Ww0bNkxhYWEmp8uexMREvfHGG9qzZ4/S0tLUsGFDPf/88ypYsKDZ0fKtRx55RIsWLdKjjz6aYdSylSaFttN2P3z4cPXq1ct1YN63b19VrlxZP/74owYPHmx2vCxzOp366aeftGXLFtecclZajMMugwgiIiLUp08fNW3aVMuWLVNCQoI2bdqkP/74Q6NGjcowmX1uskUB5O7urnPnzl1zrfzZs2ct9cf+T7Vr19bmzZv1xBNPWLrIsoP3339fCxculNPpVMOGDXXy5Em1aNFCy5cv19GjRzVgwADLrGjm5eWlzp0763//+58+/vhjOZ1OtWvXTnfddZdWrFhhmQIofbLH+Pj46640YcXr6L29vV0TwVrZxYsXFRcX5yrlzp49q4SEBJNT5V+//vqrZs6cKUn64osv1KpVKxUuXFi1a9fWmTNnTE53e1hlxFlUVJSOHj2qHj16uOYESf9MqVWrlmWKoKpVqyomJkYNGjQwO8ot69q1q5o0aZLh0pwyZcpo3rx5qlSpkonJsm/ChAm644479Oqrr0qSPvnkE40bN07Tp083OVn+tXbtWknKcDDrcDgstQy8nbb7xo0b6/PPP3d9X6tWLX399dcaPHiwJUfJDh8+XNOmTVPv3r1Vrlw5de7c+V8v089r7DCI4PTp02rbtq2kK4sLtGzZUh4eHrr//vt16dIl03LZogDq3r27+vbtqxEjRuiBBx5QgQIF9OOPP2rq1KmupaKt5Opl+9KbwfTv7XjNthUsX75c69at09mzZ9WuXTvt3btXBQoUUKdOndSxY0cNGDDAMpcdvfbaazp+/LhOnTql2bNnq0qVKnI4HAoODrbU/AZWGXGVGekrMVWrVk3vv/++nnjiiWsOPqwkLCxMQUFBqlOnjgzD0Pfff2/JiVPt4urRZHv37lVERITr+8TERDMi3bIdO3Zow4YNOnXqlNzc3OTr66smTZqoZcuWkqxz2eTSpUtVt25dlShRwjXiNzg4WGPGjNF3331ncrrMu3qJ7nRX77dY6aA2JiZGbm5u16yQV6hQIcXExFju/Vi6Mr/J6tWrXd+PHTvWNXrZTqxS/Eo3X9XMKquZ2W27/6f0E6JW3O79/f1VuXJlRUdHa8uWLXrrrbdUsmRJs2PlO+nbhmEY2rdvn+skr2EYpp4YtUUBFBwcrMuXL2vUqFE6deqUJKlcuXLq3bu3JQuggwcPSrqyJKGVJ1SzE6fTKS8vL91zzz3q3bt3hrLHaqPMZs+eLenK9bW1atXS4cOHFRsbqyeffFIlSpTQggULTE6YOenbthVH+vzT1TtRe/fuzXDwarWdKOnKe3KjRo303XffyeFwaPz48a4Rmp999pmaNWtmcsL8pUyZMlq3bp0SExOVmJjoGq6/atUqS45omD17tqKjoxUUFCRfX18ZhqHY2FgtW7ZM33//vaUunU5OTtabb76po0ePKjQ0VJUqVVJ8fLzi4+PVunVrs+Nlmp2W6O7Xr5+OHTvm+tu6mhXfj6UrBxt//fWXa9LRv/76y3KTD2/ZskXNmzeXdKU4/eKLL+Th4aEWLVq4yiyrFL//xirLXLPd5107duzQSy+9pFq1asnpdGrs2LGaNGkS+1+5rEqVKpo/f76Sk5Pl5eWlOnXqKDk5We+++65q1aplXjDDBv7880/X1+fPnzfi4uJu+jNW0bRpU2Po0KHGqlWrjAsXLpgdJ1+bNWuW0bVrVyM1NdV124EDB4wOHToYkZGRJibLvtdff/2ar8+ePWsYhmFs27bNlExZUaVKFaNq1arX/Jd+uxVdbzv/448/cj/IbRQcHGx2hHwnJibGePbZZ42QkBBjx44dhmEYxquvvmq0aNHCOHr0qLnhsiEgIMBIS0u75vbU1FSjVatWJiS6de3btzf+/vtv44cffjCaNm1qjBo1yujQoYPZsXKUVbb9ixcvGoGBgcbXX39tdpQcs2zZMiMgIMCYPHmyMXnyZKNFixbG0qVLzY6VJel/P3PmzDGeeeYZY/PmzcamTZuMPn36ZNifsYP27dubHSHHsN2bIyQkxDh+/Ljr++PHjxtBQUEmJsqf/vrrL2PcuHHGgAEDjJ9++skwDMMYN26c0b17dyM2Nta0XLYogDLz5mKVN6CrpaSkGHv37jWmTZtmBAcHG127djXmz59vdqx868svv8zw/ZEjR4zPP//cpDS3lxW3l+uxQpFlGFcO0P/880+jbdu2rq///PNP4/jx40bLli3Njpej7LRjaxXXOwESFxeXoUSx0kmSwMDA6+a18g7u5s2br/u1YVw54LUDK237P/zwgzFmzBizY9yyTz/91DAMwzh37pxx6NAhY9GiRcaHH35oHDx40ORkWZe+XxIYGGgkJSW5bk9OTjYCAgLMinVb2GUfzDDY7s0SGBh4zW3t2rUzIUn+9t133+XIz+Q0W1xfdODAAVWrVu2G9xv//1pUq/Hw8FClSpV04cIFJSUlaevWrdqwYYNlJoW0m4cffjjD935+fvLz8zMpze1l2GR+nTlz5lhiuOucOXO0b98+nTlzJsMk0B4eHnrsscfMC3YbWPG92OoGDBigqKioDLfddddd//ozedXIkSPVrVs3lS9fXj4+PnI4HDpz5oyOHTtmqWWHr5Z+acs/v5auXGYxaNCg3I6U46y07deoUcMyCyLczMyZMxUQEKA+ffooKipKlStXNjtStiUkJOjs2bMqXbq0Ll265LoUPykpiekS8jC2e3OUKVNG77//vjp27ChJWrZsme655x6TU+U/Y8aM0YIFC256XDVmzBjXpPC5xRbvmOlz5txMZq5TzWvatGmjv/76S23atJG/v7+ef/551/XbwO1kpQ/sm7FKkZV+0Dp//nz95z//ue7PMHcOsstuJ0kaNWqkDRs2KDo6WmfOnJHT6VTp0qVVs2ZNeXl5mR0vx1nlfQx5T7169fTQQw/JMIwM7wGGBRcVqVOnjnr16qWTJ09q/PjxioyM1KZNmzR58uQbfm4C+dWkSZM0ceJEzZ07V4ZhqGHDhpowYYLZsfKdhIQEde/e/aaf42bsf9miAMqMyMhIPf7442bHyJIePXpo7969+vLLL3Xu3DmdO3dODRo0UPny5c2OBliClQ5qJd10J9Yqo5mQ99jtJEn6ai333HNPhjOaZ8+elWS9VfP+jdXex5B3TJ48WZMnT1b//v319ttvmx3nlqSfKElKSlJsbKwkqXz58po7d66qVKliZrQcZ6XVzJA3lShRQrNmzdLFixfl4eGhO+64w+xI+VJm9q3SV/zOTfmmALLiGbQuXbqoS5cucjqdWr16td566y2NHz/eUmdsAOQMK7yHRUREaNCgQddcXnQ1KzyP/MhKJ0nstlpLfsFBbe7bv3+/HnzwQfXq1UtfffXVNff/89L2vOzqZbrd3d0VExOjwoULu+6zSvGbmpqqZcuWqUWLFipSpIjmz5+vH3/8UQ8++KD69eunAgUK2GY1M4nt3iyHDh3SyJEjXduNn5+fpk6dqnvvvdfkZPinJUuW5Pqq5fmmALLiGbSPP/5Ye/bsUXR0tKpWrarevXvbbj4Q5A1TpkzRyJEjXd9zkJ73WOE9bOXKldqxY4defPFFBQQEXPdnlixZksupkBlW2uYXL16srl27aty4capbt67ZcaArB7UfffSRTp48qebNm6tevXqu+yIjIzVo0CBbHdRaxeLFixUREaHIyMhr7nM4HJZ6TexS/IaHh0uSWrZsqalTpyohIUFdu3bV559/rpdeekmvvfaayQkzLywsTKNHj1a5cuVu+DNW+huzk3HjxmnIkCFq2rSpJGnz5s166aWXtGjRIpOT4Z/M2P/KNwWQFf3666/q2LGjpk+ffs28BswHguwaNWrUNbdt27ZN8fHxkq4Ms7bCQXp0dLRrsr49e/Zo+/bt8vDwUIsWLVSzZk1J1jqotYOyZctqxowZGj9+vBYsWKBevXrp8ccfl7e3t+tn0ifuRN5ihYIxXeHChRUREaGlS5fmiwKoQoUKZkf4V2PHjpXT6VTlypU1YsQIde7cWWFhYZLsM4m1FUVEREiSFi5caHKSW2eX4veXX37RmjVrJEnffPONoqKi5HA41LRpU7Vp08bkdFnzww8/qE+fPnrqqacUGhoqT09PsyPh/7t8+bKr/JGkFi1a6M033zQxEW6EOYCQwZgxY254H/OBILuKFi2qlStXKiwszDWp+N69e1W/fn3Xz1jhIH3cuHGKiorSRx99pI8//lgdOnSQdOVApFOnTurevbsliiw7cTgcqlixohYtWqTdu3dryZIlmjRpksqXL6/SpUtb6swm8jY7rdbyb2bMmGF2hH/1008/afXq1ZKk4OBg9ezZU97e3urZsydFvIlCQ0NvenBhpdEZdil+CxYsqMOHD6tSpUry8/PTyZMnVaZMGZ0+fdpyk9iXKlVK//3vfzVt2jQFBATo6aefVtu2bVltykTpl3xVrVpV8+fPV8eOHeXu7q41a9ZkGJmJ/M02BdCRI0e0ceNGnTp1Sm5ubvL19dWjjz6qhx56SJL9RgLY7fkg94SHh6tJkyaaNWuWXnjhBTVo0EAffPCBQkJCzI6WLZ988ok+/PBDFStWTJLUsWNHdezYUd27d7dEkZVux44dqlmzpu68806tXLlS0dHRevDBB13FlhW2+aszNmrUSI0aNVJKSooOHTqkP/74w8RkQN60cuXKm94fHBycKzlulWEYSkhIUMGCBVW8eHEtWLBATz/9tIoXL26p0WV2Y7eRV3YofkeOHKlevXqpTp06uuOOO9S5c2fVrFlT+/fv1yuvvGJ2vCxxOBwqWbKkpk2bpmPHjumTTz5R7969dfnyZZUuXdqUyW3zu+7du8vhcMgwDO3bty/Da+BwOG46uAD5hy0KoI8++kiffPKJWrZs6Sp8YmNj9fLLLysoKEi9e/e23UgAdqhwK/z9/VWtWjWNGzdOn3/+udLS0syOlGWpqalyOp0qWrRohrNmXl5ecnNzMzFZ1k2aNEkHDhzQzJkzNWvWLEVHR6t58+bavHmzDhw4oDFjxljiPaxbt27X3Obp6anq1aurevXqJiRCui1btmjLli2KjY2Vp6en7r33XrVu3Vq1a9eWZI2C0Y727NmjTZs2qVWrVte93yoFUPfu3RUSEqLx48fL399fpUqV0oIFC/Tss8/q3LlzZsfLt64e2fvzzz8rISFBhmEoLS1NJ06cyHA/ckft2rW1YcMG7d69W7///rvuv/9+lSxZUi+//LJKly5tdrwsufpzo3z58hoxYoRGjBihCxcucNLHJJlZdWrJkiXq0qVLLqRBZpgxUbrDsMFeX8uWLbVy5cprlrhLTExUSEiINmzYYFKy2yckJERRUVFmx4ANLF26VOvXr9e7775rdpQsCQ0N1e+//y7pymiTKVOmaM+ePZo+fboee+wxDR482OSEmde2bVutXr1a7u7uCgkJ0ZIlS+Tl5aW0tDS1a9dO69evNzsiLGzevHn6/vvv9eijj2rbtm2qV6+ePD09tWzZMvXq1UudO3fW5cuXLTVizk7CwsLUvHlzdezY0ewot+TYsWPy8vLKsBrTpUuXtGzZMvXs2dO8YNCYMWP05ZdfKj4+Xn5+fjp48KDq1Kmjd955x+xo+c7Vq5ldj1VWM5Ok7du3Z5hnBtbAMSRsMQLIw8NDqamp19yelJTEhGTAv+jUqZM6depkdowsS5/U8rffftNff/0l6cron8GDB1tutTxvb2+dO3dOvr6+Kl26tBISEuTl5aXExER5eNjibRomWrdunVauXCmHw6EOHTqob9+++vDDD9W5c2fXf5Q/5pkwYYJrUliriomJcY3E/OcB7o1WBETu2b17tzZu3KiJEyfqmWeeUWJioqZMmWJ2rHzpequZpV+yY6XVzCSpUqVKNy20rFRm5Sc2GPthGV999dVN73/44YdzKUlGtjiyCAsLU3BwsPz9/eXj4yOHw6EzZ85o7969Gjp0qNnxbgs2XuAKPz8/19dWnRhywIAB6tixo9q2bauyZcsqNDRU/v7+2rlzp5599lmz48HiLl++rMTERBUsWFBJSUmKi4uTdGUyUqtdLmk36QdPrVu3vu6BlFUOoOyyRLdd+fr6ytPTUxUqVNChQ4fUtm1bXbx40exY+ZJdVjOT2O6timlEcs+bb76p77//XjVq1LjuNmLWRPy2uARMkk6fPq09e/bozJkzcjqdKl26tOs6dKvJzPBQhusD9vLHH39oy5Yt+v3335WWlqaSJUuqWbNmlp/wEuabP3++Pv30UzVu3Fg7d+5USEiIAgIC9Nxzz6lly5bq37+/2RHzrcDAQFscQF26dMk2B7V29Pzzz+uBBx6Qv7+/pk+frqeeekqRkZG2nCLBCqKjo7V06VJNnDjR7Ci3hO3emrgELPekpKTomWee0bPPPqsnnnjC7DgutimA7MQuO4QAgLxhz549+vnnn10HgX///bdOnDihKlWqmB0tX7PTAZRdDmrt6NKlS9q+fbvatm2rhQsXas+ePXrmmWfUsGFDs6PB4tjurYcCKHcdPXpUy5cv17Bhw8yO4mKLS8Dsxk7DQwEA5vP395e/v7/r+0KFClH+5AGFCxdWRESEli5davnPezss0W1XDofDdelnQECAzp07p5o1a5obCrbAdm89Zqw6lZ8dOXJEpUqV0vHjx3Xvvfe6bjdzNTZGAOVRNOoAAAC4VWFhYapSpYqGDh2qS5cuacGCBfrtt98UGRlpdjQAOcjpdOrDDz/U1q1bFRsbK09PT917771q06aN2rZta3a8fGfGjBn66aefVKFCBW3YsEEjRoxQ+/btJZk7EosRQHkUjToAAABuVUxMjObOnSvpyqizoUOHug5CANjHlClTlJKSomeffVYbN25U1apV5evrq0WLFunYsWMaMGCA2RHzle3btysqKkoeHh4KDQ1V79695eXlpdatW5u6oBMFEAAAAGBTDodDhw4dcl32eeTIEXl4cAgA2M3evXu1evVqSdKjjz6qbt26afHixXr88ccVFBREAZTLDMNwrbpWvnx5zZs3T7169VLx4sVNXY2Nd38AAADApsLDw9W7d2/XyrgXLlzQ9OnTTU4FIKelpaXp3LlzKlGihGJjY5WUlCTpympUlL65r1WrVgoNDdXIkSNVo0YNVapUSbNnz9bAgQOVnJxsWi7mAAIAAABsLDk5Wb/88os8PDzk5+cnLy8vSeZORAogZ61YsUKzZ89W7dq19cMPP+jFF1/UQw89pJ49e2rgwIHq0KGD2RHznT179sjX11cVKlRw3Xby5Em9++67Gj16tCmZKIAAAACAfIgloQF7OXr0qA4dOqSqVauqfPnySk5OVkJCgooWLWp2NOQRjAUDAAAA8iHOAwP2ERMTowIFCrgWEoqJiXHdl5CQoDJlypgVDXkIBRAAAACQD5k5ESmAnNWvXz8dO3ZMvr6+15S7DodDW7duNSkZ8hIKIAAAAAAALGzx4sXq2rWrxo0bp7p165odB3mUm9kBAAAAAABA9hUuXFgRERFauXKl2VGQhzECCAAAAMiHihQpYnYEADmoRo0arjmAgOthBBAAAACQD7zwwgsZvv/www9NSgIAMAPLwAMAAAA2Exoaes0kzz/99JOqV68uifIHAPIjLgEDAAAAbKZly5ZasGCBnn/+eZUtW1aGYejll1/WwIEDzY4GADAJl4ABAAAANtO9e3e98847Wr58uWJiYtSgQQMVKlRI9evXV/369c2OBwAwAZeAAQAAADaVnJys119/XTExMTpy5Ig+/fRTsyMBAExCAQQAAADY3K5du/Tpp5/q1VdfNTsKAMAkFEAAAACAzcTExNz0/jJlyuRSEgBAXkEBBAAAANhMYGCgjh07Jl9fX6Xv7jscDhmGIYfDoa1bt5qcEACQ2yiAAAAAAJu5dOmSunbtqnHjxqlu3bpmxwEA5AGsAgYAAADYTOHChRUREaGVK1eaHQUAkEcwAggAAAAAAMDmGAEEAAAAAABgcxRAAAAAAAAANkcBBAAAAAAAYHMUQAAAAAAAADZHAQQAAAAAAGBz/w9qesVptwfDuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(rc={\"figure.figsize\":(20, 5)}) \n",
    "\n",
    "\n",
    "# Create a barplot showing the start word score for all of the tokens.\n",
    "ax = sns.barplot(x=[f'{i}_{t}' for i, t in enumerate(token_labels)], y=output.start_logits.squeeze().tolist(), ci=None)\n",
    "# Turn the xlabels vertical.\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "# Turn on the vertical grid to help align words to scores.\n",
    "plt.title('Start Word Scores')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Create a barplot showing the start word score for all of the tokens.\n",
    "ax = sns.barplot(x=[f'{i}_{t}' for i, t in enumerate(token_labels)], y=output.end_logits.squeeze().tolist(), ci=None)\n",
    "# Turn the xlabels vertical.\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "# Turn on the vertical grid to help align words to scores.\n",
    "plt.title('End Word Scores')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8804d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ff9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
