{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktxdLosxtgZd"
   },
   "source": [
    "# Recommender Systems with Graph Neural Networks in PyG\n",
    "\n",
    "By Derrick Li, Peter Maldonado, Akram Sbaih as part of the Stanford CS224W course project.\n",
    "\n",
    "In this tutorial, we implement two GNN recommender system architectures, LightGCN and NGCF, in PyG and apply them to the MovieLens 100K dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________\n",
    "发现这个版本写的好 clean 啊: (LightGCN 写的短，然后有用label encoder, 然后 bipartite 也有处理)\n",
    "- blog: https://medium.com/stanford-cs224w/recommender-systems-with-gnns-in-pyg-d8301178e377\n",
    "- code: https://colab.research.google.com/drive/1VQTBxJuty7aLMepjEYE-d7E9kjo51CA1?usp=sharing#scrollTo=oHuXurG8mezC\n",
    "\n",
    "而且他也是有考虑进去 bipartite graph, 但是没有像我那样按公式硬转， 而是我之前的 offset 写法， 那我看一下他 evaluation 的时候咋写的\n",
    "```\n",
    "u_t = torch.LongTensor(train_df.user_id_idx)\n",
    "i_t = torch.LongTensor(train_df.item_id_idx) + n_users\n",
    "\n",
    "train_edge_index = torch.stack((\n",
    "  torch.cat([u_t, i_t]),\n",
    "  torch.cat([i_t, u_t])\n",
    ")).to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里还有个类似的，如果上面这个又问题可以做参考对比:\n",
    "- [Medium][Implementing Neural Graph Collaborative Filtering in PyTorch](https://medium.com/@yusufnoor_88274/implementing-neural-graph-collaborative-filtering-in-pytorch-4d021dff25f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "我发现他这里，也是对 training 和 BPR 用的是 bipartite adj mat 的 edge_list\n",
    "\n",
    "然后对 recall 用的是正常的 interaciton mat 的 edge_list， 那就说明我其实也可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoRvTQ1vtwcq"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, we'll install the necessary packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p_Pj_D7t3Rk"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_LPed5cuAEa"
   },
   "source": [
    "Next, let's import all of the modules that we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y9fonQcxt3do"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "import torch_geometric\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.data import download_url, extract_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzLUutf7uNAS"
   },
   "source": [
    "Lastly, we should double check that our environment is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J_CDy1cbuF4_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b4pKT5jUt3pz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYnQc9UH07Fg"
   },
   "source": [
    "## Dataset and Preprocessing\n",
    "\n",
    "For this tutorial, we’ll be using the [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/), which contains 100,000 ratings by 943 users of 1682 items (movies). To ensure the quality of the dataset, each user has rated at least 20 movies. \n",
    "\n",
    "We’ll focus on the interactions between users and items, in this case user ratings of movies, but the dataset also provides metadata about users and movies, such as user demographics and movie titles, release dates, and genres. T\n",
    "\n",
    "The user ratings of movies form a bipartite graph, which we can apply graph machine learning methods to recommend new movies to users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "# extract_zip(download_url(url, '.'), '.')\n",
    "\n",
    "movie_path = './ml-latest-small/movies.csv'\n",
    "rating_path = './ml-latest-small/ratings.csv'\n",
    "user_path = './ml-latest-small/users.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating df 用新版的 \n",
    "# url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "# Small: 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users. Last updated 9/2018\n",
    "# 这个 density 低很多的\n",
    "# 原来的 .data 板的 density 很高， 能到 precision / recall 嫩到 (0.2395, 0.3564)\n",
    "# 看下新的咋样\n",
    "df = pd.read_csv(rating_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "D13_omigmeOi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100836\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# columns_name=['user_id','item_id','rating','timestamp']\n",
    "# df = pd.read_csv(\"./ml-100k/u.data\",sep=\"\\t\",names=columns_name)\n",
    "print(len(df))\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzX3-JClrcCx"
   },
   "source": [
    "We only want to use high ratings as interactions in order to predict which movies a user will enjoy watching next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610\n",
      "9724\n"
     ]
    }
   ],
   "source": [
    "num_users = len(df['userId'].unique())\n",
    "num_movies = len(df['movieId'].unique())\n",
    "\n",
    "print(num_users)\n",
    "print(num_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yvuk3tEomrQI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating Distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rating\n",
       "0.5     1370\n",
       "1.0     2811\n",
       "1.5     1791\n",
       "2.0     7551\n",
       "2.5     5550\n",
       "3.0    20047\n",
       "3.5    13136\n",
       "4.0    26818\n",
       "4.5     8551\n",
       "5.0    13211\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the distribution of highly rated movies?\n",
    "print(\"Rating Distribution\")\n",
    "df.groupby(['rating'])['rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SWwvL8JOmrT4"
   },
   "outputs": [],
   "source": [
    "# Perform a 80/20 train-test split on the interactions in the dataset\n",
    "train, test = train_test_split(df.values, test_size=0.2, random_state=16)\n",
    "train_df = pd.DataFrame(train, columns=df.columns)\n",
    "test_df = pd.DataFrame(test, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "50eSoP3qmrbJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size  :  80668\n",
      "Test Size :  20168\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Size  : \", len(train_df))\n",
    "print(\"Test Size : \", len (test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0bHVYGjrTcL"
   },
   "source": [
    "Since we performed the train/test randomly on the interactions, not all users and items may be present in the training set. We will relabel all of users and items to ensure the highest label is the number of users and items, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QXi90opJmriQ"
   },
   "outputs": [],
   "source": [
    "le_user = pp.LabelEncoder()\n",
    "le_item = pp.LabelEncoder()\n",
    "train_df['user_id_idx'] = le_user.fit_transform(train_df['userId'].values)\n",
    "train_df['item_id_idx'] = le_item.fit_transform(train_df['movieId'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KtRmOkoDmem_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610 8959\n",
      "20168\n"
     ]
    }
   ],
   "source": [
    "train_user_ids = train_df['userId'].unique()\n",
    "train_item_ids = train_df['movieId'].unique()\n",
    "\n",
    "print(len(train_user_ids), len(train_item_ids))\n",
    "\n",
    "test_df = test_df[\n",
    "  (test_df['userId'].isin(train_user_ids)) & \\\n",
    "  (test_df['movieId'].isin(train_item_ids))\n",
    "]\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里是对 test_df 用同样的 transform? 我忘记原因是啥？\n",
    "# \"If you fit() to your test data, you'd compute a new mean and variance for each feature.\"\n",
    "# https://stackoverflow.com/questions/48692500/fit-transform-on-training-data-and-transform-on-test-data\n",
    "# 就是用 training set the statistics 来 transform test set 但是 不能重新 fit 一次 test set. Fit 是重新算 statistics (mean, vairance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9fKAfWyCm5eY"
   },
   "outputs": [],
   "source": [
    "test_df['user_id_idx'] = le_user.transform(test_df['userId'].values)\n",
    "test_df['item_id_idx'] = le_item.transform(test_df['movieId'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-WOF-cOAm5iO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Users :  610\n",
      "Number of unique Items :  8959\n",
      "609\n",
      "8958\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "n_users = train_df['user_id_idx'].nunique()\n",
    "n_items = train_df['item_id_idx'].nunique()\n",
    "print(\"Number of Unique Users : \", n_users)\n",
    "print(\"Number of unique Items : \", n_items)\n",
    "\n",
    "\n",
    "print(train_df['user_id_idx'].max())\n",
    "print(train_df['item_id_idx'].max())\n",
    "print(train_df['item_id_idx'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNoblY5kxlv_"
   },
   "source": [
    "### Minibatch Sampling\n",
    "\n",
    "Explain the scheme of minibatch positive and negative sample in some amount of prose.\n",
    "\n",
    "We need to add `n_usr` to the sampled positive and negative items, since each node must have a unique id when using PyG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NQRGy-CJnOkg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 40,  92, 141, 190, 203, 271, 294, 317, 364, 372, 409, 430, 466, 530,\n",
       "         567, 603]),\n",
       " tensor([4089, 1759,  937,  662, 7815, 6612, 1364, 3000, 9124,  729, 3662, 1073,\n",
       "         2353, 1290, 2323,  751]),\n",
       " tensor([7175, 4834, 5409, 6325, 5558, 8328, 2864, 3942, 1735, 7013, 6413, 3481,\n",
       "         1146, 8411, 4848, 4932]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里有考虑 bipartite 对 edge_index 的影响吗？\n",
    "def data_loader(data, batch_size, n_usr, n_itm):\n",
    "\n",
    "    def sample_neg(x):\n",
    "        while True:\n",
    "            neg_id = random.randint(0, n_itm - 1)\n",
    "            if neg_id not in x:\n",
    "                return neg_id\n",
    "\n",
    "    interected_items_df = data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n",
    "    indices = [x for x in range(n_usr)]\n",
    "\n",
    "    if n_usr < batch_size:\n",
    "        users = [random.choice(indices) for _ in range(batch_size)]\n",
    "    else:\n",
    "        users = random.sample(indices, batch_size)\n",
    "        \n",
    "    users.sort()\n",
    "    users_df = pd.DataFrame(users,columns = ['users'])\n",
    "\n",
    "    interected_items_df = pd.merge(interected_items_df, users_df, how = 'right', left_on = 'user_id_idx', right_on = 'users')\n",
    "    pos_items = interected_items_df['item_id_idx'].apply(lambda x : random.choice(x)).values\n",
    "    neg_items = interected_items_df['item_id_idx'].apply(lambda x: sample_neg(x)).values\n",
    "\n",
    "    return (\n",
    "        torch.LongTensor(list(users)).to(device), \n",
    "        torch.LongTensor(list(pos_items)).to(device) + n_usr, \n",
    "        torch.LongTensor(list(neg_items)).to(device) + n_usr\n",
    "    )\n",
    "\n",
    "data_loader(train_df, 16, n_users, n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjHZg1Eu-MKs"
   },
   "source": [
    "## Edge Index\n",
    "\n",
    "PyG represents graphs as sparse lists of node pairs. Since our graph is undirected, we need to include each edge twice, once for the edges from the users to the items and vice-versa.\n",
    "\n",
    "Similar to above, we add `n_users` to the item tensor to ensure that every node in the graph has a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "O3BkGyV9pkce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 209,  293,  598,  ..., 2496, 1568, 7569],\n",
      "        [9216, 1637, 1158,  ...,  325,  607,   17]])\n",
      "torch.Size([2, 161336])\n",
      "\n",
      "tensor(9568)\n",
      "tensor(0)\n",
      "\n",
      "tensor(9568)\n",
      "tensor(610)\n"
     ]
    }
   ],
   "source": [
    "#喜欢他这里的 bipartite 的处理，我之前的处理是转成 matrix 再转回来\n",
    "# 他这里直接靠 offset， 但是他 eval 的时候，index 会不会有点问题?\n",
    "u_t = torch.LongTensor(train_df.user_id_idx)\n",
    "i_t = torch.LongTensor(train_df.item_id_idx) + n_users\n",
    "\n",
    "train_edge_index = torch.stack((\n",
    "  torch.cat([u_t, i_t]),\n",
    "  torch.cat([i_t, u_t])\n",
    ")).to(device)\n",
    "\n",
    "# 这里应该就已经 做了 bipartite 处理了？因为 train_df size 是 80668\n",
    "print(train_edge_index)\n",
    "print(train_edge_index.size())\n",
    "\n",
    "# 看一下 item id 有没有变化?\n",
    "print()\n",
    "print(train_edge_index.max())\n",
    "print(train_edge_index.min())\n",
    "\n",
    "# 看下这个, 有，609 + 8958 = 9568 \n",
    "# 也就是这里确实是已经做了 bipartite 的 adj_mat 处理了\n",
    "print()\n",
    "print(i_t.max())\n",
    "print(i_t.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_t_test = torch.LongTensor(test_df.user_id_idx)\n",
    "i_t_test = torch.LongTensor(test_df.item_id_idx) + n_users\n",
    "\n",
    "test_edge_index = torch.stack((\n",
    "  torch.cat([u_t_test, i_t_test]),\n",
    "  torch.cat([i_t_test, u_t_test])\n",
    ")).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_--------------------- v--------------------_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check density\n",
    "sparse_train_edge_index = SparseTensor(row=train_edge_index[0], \n",
    "                                       col=train_edge_index[1], \n",
    "                                       sparse_sizes=((n_users + n_items), (n_users + n_items)))\n",
    "\n",
    "# 卧槽，怪不得这个 recall 这么搞，原版的能到 30% 尼玛 density 2.13， 我的那个版本只有 density=0.15%\n",
    "print(sparse_train_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_--------------------- ^ --------------------_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RxDUYJ2sXJe"
   },
   "source": [
    "Let's confirm that the first and last edges match the middle two edges, but with the order of nodes swapped.\n",
    "\n",
    "这里就是确认一下上面的 bipartite 处理是可以的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Mq4NVs0_nOxh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7569,   17]), tensor([ 209, 9216]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_edge_index[:,-1], train_edge_index[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_gwESDz-qgw2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([  17, 7569]), tensor([9216,  209]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里 len(train) 是 bipartite 处理前的 length 所以就相当于中间\n",
    "print(len(train))\n",
    "train_edge_index[:, len(train)-1], train_edge_index[:, len(train)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ys1P7mtcr54"
   },
   "source": [
    "## Model Architecture\n",
    "\n",
    "First, let's take a look at the graph convolutional layers that will power our recommender system GNN. Then, we can implement a wrapper to stack multiple convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49WD8SryyUds"
   },
   "source": [
    "### LightGCN Convolutional Layer\n",
    "\n",
    "The LightGCN architecture is governed by the following rules:\n",
    "\n",
    "$$e_{u}^{(k+1)} = \\sum\\limits_{i \\in N_u} \\frac{1}{\\sqrt{|N_u|}\\sqrt{|N_i|}}e^{(k)}_i$$\n",
    "\n",
    "$$e_{i}^{(k+1)} = \\sum\\limits_{u \\in N_i} \\frac{1}{\\sqrt{|N_i|}\\sqrt{|N_u|}}e^{(k)}_u$$\n",
    "In essence, the embedding for each node after a single LightGCN layer is the sum of the synthetic normalized embeddings of it's neighbors before the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcUsEulPtNNp"
   },
   "source": [
    "\n",
    "Briefly explain how the `MessagePassing` class works (look at colabs)\n",
    "\n",
    "We can specify the type of aggregation our `MessagePassing` layer should use by passing in an `aggr=` argument in the layer initialization. Here we use `add` to specify summation aggregation of messages.\n",
    "\n",
    "Note that we could have manually defined our aggregation function by defining a function explicitly in the class:\n",
    "```\n",
    "def aggregate(self, x, messages, index):\n",
    "  return torch_scatter.scatter(messages, index, self.node_dim, reduce=\"sum\")\n",
    "```\n",
    "The `torch_scatter.scatter` function enables us to aggregate messages being sent to the same node. The `reduce=` argument specifies how to aggregate, while `index` has the same length as the `messages` tensor and maps from message to destination node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "哇，他这个写的好精简呀, 而且是比较标准，没有用 gcn_noam 和 sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-aTMoHisNIh_"
   },
   "outputs": [],
   "source": [
    "class LightGCNConv(MessagePassing):\n",
    "  def __init__(self, **kwargs):  \n",
    "    super().__init__(aggr='add')\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    # Compute normalization\n",
    "    # 啊，原来 GCN norm 的计算是这么写的, 懂了\n",
    "    from_, to_ = edge_index\n",
    "    deg = degree(to_, x.size(0), dtype=x.dtype)\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n",
    "\n",
    "    # Start propagating messages (no update after aggregation)\n",
    "    return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "  def message(self, x_j, norm):\n",
    "    return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0Lrwz-4yei9"
   },
   "source": [
    "Let's test out our implementation of the LightGCN convolution by applying it to a small bipartite graph. \n",
    "\n",
    "This sample graph is undirected, and node 0 is connected to nodes 2 and 3 while node 1 is connected to 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "bgcrWvgkhxQR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.7071, 0.5000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.5000, 0.7071],\n",
       "        [0.7071, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.7071, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = torch.Tensor(np.eye(5))\n",
    "test_edge_index = torch.LongTensor(np.array([\n",
    "  [0, 0, 1, 1, 2, 3, 3, 4],\n",
    "  [2, 3, 3, 4, 0, 0, 1, 1]\n",
    "]))\n",
    "\n",
    "LightGCNConv()(test_x, test_edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3CtZKN-yvIQ"
   },
   "source": [
    "Notice how each node has an initial feature vector corresponding to a one-hot encoding at the index of their id.\n",
    "\n",
    "As we expected, node 0 received messages (and so has non-zero features at the corresponding indicies) from nodes 2 and 3. We can easily verify that nodes 1, 2, 3, and 4 also received messages from their precisely neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BIVMYyiyPHJ"
   },
   "source": [
    "### NGCF Layer\n",
    "\n",
    "NGCF is an older architecture than LightGCN that originated by researchers who applied [Graph Convolutional Networks (GCNs)]() to recommender systems. LightGCN functions the same as NGCF, but removes the learnable linear layers, non-linear activation, and dropout.\n",
    "\n",
    "One layer of NGCF updates user and item embeddings as follows:\n",
    "\n",
    "$$e_{u}^{(k+1)} = \\sigma\\left(W_1 e_u^{(k)} + \\sum\\limits_{i \\in N_u} \\frac{1}{\\sqrt{|N_u|}\\sqrt{|N_i|}}(W_1e^{(k)}_i + W_2(e^{(k)}_i \\odot e^{(k)}_u))\\right)$$\n",
    "\n",
    "$$e_{i}^{(k+1)} = \\sigma\\left(W_1 e_i^{(k)} + \\sum\\limits_{u \\in N_i} \\frac{1}{\\sqrt{|N_i|}\\sqrt{|N_u|}}(W_1e^{(k)}_u + W_2(e^{(k)}_u \\odot e^{(k)}_i))\\right)$$\n",
    "\n",
    "Typically, NGCF is implemented with dropout before the activation and with an activation function $\\sigma$ of LeakyReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "u728UyYfOczG"
   },
   "outputs": [],
   "source": [
    "class NGCFConv(MessagePassing):\n",
    "  def __init__(self, latent_dim, dropout, bias=True, **kwargs):  \n",
    "    super(NGCFConv, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "    self.dropout = dropout\n",
    "\n",
    "    self.lin_1 = nn.Linear(latent_dim, latent_dim, bias=bias)\n",
    "    self.lin_2 = nn.Linear(latent_dim, latent_dim, bias=bias)\n",
    "\n",
    "    self.init_parameters()\n",
    "\n",
    "\n",
    "  def init_parameters(self):\n",
    "    nn.init.xavier_uniform_(self.lin_1.weight)\n",
    "    nn.init.xavier_uniform_(self.lin_2.weight)\n",
    "\n",
    "\n",
    "  def forward(self, x, edge_index):\n",
    "    # Compute normalization\n",
    "    from_, to_ = edge_index\n",
    "    deg = degree(to_, x.size(0), dtype=x.dtype)\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    norm = deg_inv_sqrt[from_] * deg_inv_sqrt[to_]\n",
    "\n",
    "    # Start propagating messages\n",
    "    out = self.propagate(edge_index, x=(x, x), norm=norm)\n",
    "\n",
    "    # Perform update after aggregation\n",
    "    out += self.lin_1(x)\n",
    "    out = F.dropout(out, self.dropout, self.training)\n",
    "    return F.leaky_relu(out)\n",
    "\n",
    "\n",
    "  def message(self, x_j, x_i, norm):\n",
    "    return norm.view(-1, 1) * (self.lin_1(x_j) + self.lin_2(x_j * x_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2tW9FJFqNjn"
   },
   "source": [
    "### Recommender System GNN\n",
    "\n",
    "For this tutorial, we will be using the following class, `RecSysGNN` in order to stack the NGCF or LightGCN convolutional layers. Some considerations that can be made for tweaking the models are the number of layers of your model and dropout. The more number of layers you add to the model, the more your model will \"diffuse\" information of recommendations made from nodes that are `n`-hops away in a model that uses `n` layers. Dropout can be tweaked to try out different regularization schemes.\n",
    "\n",
    "Notice that our forward function works differently from most neural networks by forward propagating embeddings for all nodes in the graph. This is because the embeddings for a given node depend on the embeddings of it's `n`-hop neighborhood, so they need to be propagated as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nT5LTkI8Ml1c"
   },
   "outputs": [],
   "source": [
    "class RecSysGNN(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      latent_dim, \n",
    "      num_layers,\n",
    "      num_users,\n",
    "      num_items,\n",
    "      model, # 'NGCF' or 'LightGCN'\n",
    "      dropout=0.1 # Only used in NGCF\n",
    "  ):\n",
    "    super(RecSysGNN, self).__init__()\n",
    "\n",
    "    assert (model == 'NGCF' or model == 'LightGCN'), 'Model must be NGCF or LightGCN'\n",
    "    self.model = model\n",
    "    self.embedding = nn.Embedding(num_users + num_items, latent_dim) # latent_dim 这个就是 vector len 貌似都是用 64\n",
    "\n",
    "    if self.model == 'NGCF':\n",
    "      self.convs = nn.ModuleList(\n",
    "        NGCFConv(latent_dim, dropout=dropout) for _ in range(num_layers)\n",
    "      )\n",
    "    else:\n",
    "      self.convs = nn.ModuleList(LightGCNConv() for _ in range(num_layers))\n",
    "\n",
    "    self.init_parameters()\n",
    "\n",
    "\n",
    "  def init_parameters(self):\n",
    "    if self.model == 'NGCF':\n",
    "      nn.init.xavier_uniform_(self.embedding.weight, gain=1)\n",
    "    else:\n",
    "      # Authors of LightGCN report higher results with normal initialization\n",
    "      nn.init.normal_(self.embedding.weight, std=0.1) \n",
    "\n",
    "\n",
    "  # 看下这里有没有对 bipartite adj mat edge_index 做别的处理? \n",
    "  # 没有，就是标准写法\n",
    "  def forward(self, edge_index):\n",
    "    emb0 = self.embedding.weight\n",
    "    embs = [emb0]\n",
    "\n",
    "    emb = emb0\n",
    "    for conv in self.convs:\n",
    "      emb = conv(x=emb, edge_index=edge_index)\n",
    "      embs.append(emb)\n",
    "\n",
    "    out = (\n",
    "      torch.cat(embs, dim=-1) if self.model == 'NGCF' \n",
    "      else torch.mean(torch.stack(embs, dim=0), dim=0)\n",
    "    )\n",
    "    \n",
    "    return emb0, out\n",
    "\n",
    "\n",
    "  def encode_minibatch(self, users, pos_items, neg_items, edge_index):\n",
    "    # 拿 bipartite adj mat 处理过的 edge_index 自己过一遍，得到 emb0 和 emb_final\n",
    "    emb0, out = self(edge_index)\n",
    "    return (\n",
    "        out[users], \n",
    "        out[pos_items], \n",
    "        out[neg_items], \n",
    "        emb0[users],\n",
    "        emb0[pos_items],\n",
    "        emb0[neg_items]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyqEQ6kfCY5V"
   },
   "source": [
    "## Loss function and metrics\n",
    "\n",
    "We implement both the Bayesian Personalized Ranking loss function for a single minibatch of users, positive items, and negative items, as well as the precision@K and recall@K metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bwrPmvXPow5q"
   },
   "outputs": [],
   "source": [
    "def compute_bpr_loss(users, users_emb, pos_emb, neg_emb, user_emb0,  pos_emb0, neg_emb0):\n",
    "    # compute loss from initial embeddings, used for regulization\n",
    "    reg_loss = (1 / 2) * (\n",
    "    user_emb0.norm().pow(2) + \n",
    "    pos_emb0.norm().pow(2)  +\n",
    "    neg_emb0.norm().pow(2)\n",
    "    ) / float(len(users))\n",
    "\n",
    "    # compute BPR loss from user, positive item, and negative item embeddings\n",
    "    pos_scores = torch.mul(users_emb, pos_emb).sum(dim=1)\n",
    "    neg_scores = torch.mul(users_emb, neg_emb).sum(dim=1)\n",
    "\n",
    "    bpr_loss = torch.mean(F.softplus(neg_scores - pos_scores))\n",
    "\n",
    "    return bpr_loss, reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面这个估计得 explore 一下？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "oHuXurG8mezC"
   },
   "outputs": [],
   "source": [
    "# 这里完全没有用到 train_data.. \n",
    "def get_metrics(user_Embed_wts, item_Embed_wts, n_users, n_items, train_data, test_data, K):\n",
    "  \n",
    "    test_user_ids = torch.LongTensor(test_data['user_id_idx'].unique())\n",
    "  \n",
    "    # compute the score of all user-item pairs\n",
    "    relevance_score = torch.matmul(user_Embed_wts, torch.transpose(item_Embed_wts,0, 1))\n",
    "\n",
    "    # create dense tensor of all user-item interactions\n",
    "    # 这是啥？\n",
    "    i = torch.stack((\n",
    "        torch.LongTensor(train_df['user_id_idx'].values),\n",
    "        torch.LongTensor(train_df['item_id_idx'].values)\n",
    "    ))\n",
    "    \n",
    "    # 这里居然直接用的是 train_df??\n",
    "    v = torch.ones((len(train_df)), dtype=torch.float64)\n",
    "    \n",
    "    # 这个是咋弄的? 哦, sparse tensor 就是按照 COO 格式来 init， 诶，那怎么 v 全部用 1 呢？？\n",
    "    # 下面有一个 (1 - interactions_t)?? 这里有点乱其实.. 这个是为了 masking?? 因为上面是  train data\n",
    "    # 这边还是没有看到 bipartite 和 train / test data 的 index 的影响, 我觉得大概率是直接减？\n",
    "    # ?? 这里 n_items 是 train data 不是全部的 n_items...\n",
    "    interactions_t = torch.sparse.FloatTensor(i, v, (n_users, n_items)).to_dense().to(device)\n",
    "\n",
    "    # mask out training user-item interactions from metric computation\n",
    "    relevance_score = torch.mul(relevance_score, (1 - interactions_t))\n",
    "\n",
    "    # compute top scoring items for each user\n",
    "    topk_relevance_indices = torch.topk(relevance_score, K).indices\n",
    "    topk_relevance_indices_df = pd.DataFrame(topk_relevance_indices.cpu().numpy() ,columns =['top_indx_'+str(x+1) for x in range(K)])\n",
    "    topk_relevance_indices_df['user_ID'] = topk_relevance_indices_df.index\n",
    "    topk_relevance_indices_df['top_rlvnt_itm'] = topk_relevance_indices_df[['top_indx_'+str(x+1) for x in range(K)]].values.tolist()\n",
    "    topk_relevance_indices_df = topk_relevance_indices_df[['user_ID','top_rlvnt_itm']]\n",
    "\n",
    "    # measure overlap between recommended (top-scoring) and held-out user-item \n",
    "    # interactions\n",
    "    test_interacted_items = test_data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n",
    "    metrics_df = pd.merge(test_interacted_items,topk_relevance_indices_df, how= 'left', left_on = 'user_id_idx',right_on = ['user_ID'])\n",
    "    metrics_df['intrsctn_itm'] = [list(set(a).intersection(b)) for a, b in zip(metrics_df.item_id_idx, metrics_df.top_rlvnt_itm)]\n",
    "\n",
    "    metrics_df['recall'] = metrics_df.apply(lambda x : len(x['intrsctn_itm'])/len(x['item_id_idx']), axis = 1) \n",
    "    metrics_df['precision'] = metrics_df.apply(lambda x : len(x['intrsctn_itm'])/K, axis = 1)\n",
    "\n",
    "    return metrics_df['recall'].mean(), metrics_df['precision'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qOC3fF9m6cH"
   },
   "source": [
    "## Train and evaluate models\n",
    "\n",
    "Now that we've implemented both LightGCN and NGCF in PyG, we can train and evaluate their performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "MZtgfxxIm5nL"
   },
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "n_layers = 3 \n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "DECAY = 0.0001\n",
    "LR = 0.005 \n",
    "K = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert bipartite adj mat based edge index to interaction mat based edge_index\n",
    "# 跟上面的是反的，就是为了方便 算 BPR 和 recall 的时候，可以 semantically  用 row0 是 user, row1 是 item\n",
    "def convert_adj_mat_edge_index_to_r_mat_edge_index(input_edge_index):\n",
    "    # 得先定义一个 sparse 不然没法转 dense matrix\n",
    "    sparse_input_edge_index = SparseTensor(row=input_edge_index[0], \n",
    "                                           col=input_edge_index[1], \n",
    "                                           sparse_sizes=((num_users + num_movies), num_users + num_movies))\n",
    "    adj_mat = sparse_input_edge_index.to_dense()\n",
    "    interact_mat = adj_mat[: num_users, num_users :]\n",
    "    r_mat_edge_index = interact_mat.to_sparse_coo().indices()\n",
    "    return r_mat_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_mat_train_edge_index = convert_adj_mat_edge_index_to_r_mat_edge_index(train_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "B5HB_FX5pdgv"
   },
   "outputs": [],
   "source": [
    "# 看下这里有没有什么特殊处理对 bipartite adj mat egdge list?\n",
    "# 这里传的是 train_df 不是 train_edge_index?? 看下下面怎么弄？？\n",
    "def train_and_eval(model, optimizer, train_df):\n",
    "  loss_list_epoch = []\n",
    "  bpr_loss_list_epoch = []\n",
    "  reg_loss_list_epoch = []\n",
    "\n",
    "  recall_list = []\n",
    "  precision_list = []\n",
    "\n",
    "  for epoch in tqdm(range(EPOCHS)):\n",
    "      n_batch = int(len(train)/BATCH_SIZE)\n",
    "    \n",
    "      final_loss_list = []\n",
    "      bpr_loss_list = []\n",
    "      reg_loss_list = []\n",
    "    \n",
    "      model.train()\n",
    "      for batch_idx in range(n_batch):\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          # ok, 这里是用 train_df 也就是没有 bipartite adj mat 处理的, 但是得到 user_item pair..  \n",
    "          users, pos_items, neg_items = data_loader(train_df, BATCH_SIZE, n_users, n_items)\n",
    "          \n",
    "          # 这里才是去拿到 embedding， 这里用的，就是 bipartite 处理过的 train_edge_index 了  \n",
    "          # 也就是说他这里 bpr loss 也是跟我一样用 bipartite 处理过的 train_edge_index 来弄\n",
    "          users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0 = model.encode_minibatch(users, \n",
    "                                                                                            pos_items, \n",
    "                                                                                            neg_items, \n",
    "                                                                                            train_edge_index)\n",
    "\n",
    "          bpr_loss, reg_loss = compute_bpr_loss(users, \n",
    "                                                users_emb, \n",
    "                                                pos_emb, \n",
    "                                                neg_emb, \n",
    "                                                userEmb0,  \n",
    "                                                posEmb0, \n",
    "                                                negEmb0)\n",
    "        \n",
    "          reg_loss = DECAY * reg_loss\n",
    "          final_loss = bpr_loss + reg_loss\n",
    "\n",
    "          final_loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          final_loss_list.append(final_loss.item())\n",
    "          bpr_loss_list.append(bpr_loss.item())\n",
    "          reg_loss_list.append(reg_loss.item())\n",
    "\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "          _, out = model(train_edge_index)\n",
    "          final_user_Embed, final_item_Embed = torch.split(out, (n_users, n_items))\n",
    "          \n",
    "          \n",
    "          test_topK_recall,  test_topK_precision = get_metrics(final_user_Embed, \n",
    "                                                               final_item_Embed, \n",
    "                                                               n_users, \n",
    "                                                               n_items, \n",
    "                                                               train_df, \n",
    "                                                               test_df, \n",
    "                                                               K\n",
    "          )\n",
    "              \n",
    "            \n",
    "          print(f\"epoch-batch_idx: {epoch}-{batch_idx}: train_loss: {final_loss.item()}   test_topK_recall {test_topK_recall}, test_topK_precision {test_topK_precision}\")\n",
    "\n",
    "      loss_list_epoch.append(round(np.mean(final_loss_list),4))\n",
    "      bpr_loss_list_epoch.append(round(np.mean(bpr_loss_list),4))\n",
    "      reg_loss_list_epoch.append(round(np.mean(reg_loss_list),4))\n",
    "\n",
    "      recall_list.append(round(test_topK_recall,4))\n",
    "      precision_list.append(round(test_topK_precision,4))\n",
    "\n",
    "  return (\n",
    "    loss_list_epoch, \n",
    "    bpr_loss_list_epoch, \n",
    "    reg_loss_list_epoch, \n",
    "    recall_list, \n",
    "    precision_list\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4xJSiBiznki"
   },
   "source": [
    "### Train and eval LightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users: 610, n_items 8959\n"
     ]
    }
   ],
   "source": [
    "print(f\"n_users: {n_users}, n_items {n_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "eKBv9eXongux"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Learnable Embedding :  [torch.Size([9569, 64])]\n"
     ]
    }
   ],
   "source": [
    "lightgcn = RecSysGNN(\n",
    "    latent_dim=latent_dim, \n",
    "    num_layers=n_layers,\n",
    "    num_users=n_users,\n",
    "    num_items=n_items,\n",
    "    model='LightGCN'\n",
    ")\n",
    "lightgcn.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lightgcn.parameters(), lr=LR)\n",
    "print(\"Size of Learnable Embedding : \", [x.shape for x in list(lightgcn.parameters())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXfsuJlcy3FT"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c58a89d44e4259bf3ac529decae1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch-batch_idx: 0-77: train_loss: 0.24155192077159882   test_topK_recall 0.1347642946693434, test_topK_precision 0.13565573770491804\n",
      "epoch-batch_idx: 1-77: train_loss: 0.20748399198055267   test_topK_recall 0.1512197823357172, test_topK_precision 0.1483606557377049\n",
      "epoch-batch_idx: 2-77: train_loss: 0.18385258316993713   test_topK_recall 0.17168730371888805, test_topK_precision 0.16303278688524592\n",
      "epoch-batch_idx: 3-77: train_loss: 0.1607300490140915   test_topK_recall 0.17566346171070366, test_topK_precision 0.16459016393442624\n",
      "epoch-batch_idx: 4-77: train_loss: 0.17203058302402496   test_topK_recall 0.17420862291712214, test_topK_precision 0.16311475409836065\n",
      "epoch-batch_idx: 5-77: train_loss: 0.15099895000457764   test_topK_recall 0.17733222136624502, test_topK_precision 0.1648360655737705\n",
      "epoch-batch_idx: 6-77: train_loss: 0.11276644468307495   test_topK_recall 0.17940750947972586, test_topK_precision 0.16696721311475413\n",
      "epoch-batch_idx: 7-77: train_loss: 0.11198142170906067   test_topK_recall 0.17969243452280995, test_topK_precision 0.16868852459016392\n",
      "epoch-batch_idx: 8-77: train_loss: 0.10789956897497177   test_topK_recall 0.1867533528578947, test_topK_precision 0.17311475409836066\n",
      "epoch-batch_idx: 9-77: train_loss: 0.1129503920674324   test_topK_recall 0.1894537098104785, test_topK_precision 0.1737704918032787\n",
      "epoch-batch_idx: 10-77: train_loss: 0.09096457809209824   test_topK_recall 0.19280309322067063, test_topK_precision 0.17647540983606558\n",
      "epoch-batch_idx: 11-77: train_loss: 0.09624335169792175   test_topK_recall 0.19551325537686323, test_topK_precision 0.1786065573770492\n",
      "epoch-batch_idx: 12-77: train_loss: 0.10002729296684265   test_topK_recall 0.1998252284864986, test_topK_precision 0.18049180327868855\n",
      "epoch-batch_idx: 13-77: train_loss: 0.08875838667154312   test_topK_recall 0.20249951149846993, test_topK_precision 0.1830327868852459\n",
      "epoch-batch_idx: 14-77: train_loss: 0.0820372924208641   test_topK_recall 0.20561085966901238, test_topK_precision 0.18491803278688523\n",
      "epoch-batch_idx: 15-77: train_loss: 0.09003714472055435   test_topK_recall 0.20883914000513817, test_topK_precision 0.18827868852459015\n",
      "epoch-batch_idx: 16-77: train_loss: 0.08563137799501419   test_topK_recall 0.20856993156186865, test_topK_precision 0.1898360655737705\n",
      "epoch-batch_idx: 17-77: train_loss: 0.06772579252719879   test_topK_recall 0.21855568854062635, test_topK_precision 0.1944262295081967\n",
      "epoch-batch_idx: 18-77: train_loss: 0.06771183013916016   test_topK_recall 0.2238714863340425, test_topK_precision 0.1987704918032787\n",
      "epoch-batch_idx: 19-77: train_loss: 0.055163513869047165   test_topK_recall 0.22793768422436883, test_topK_precision 0.20262295081967213\n",
      "epoch-batch_idx: 20-77: train_loss: 0.07844562083482742   test_topK_recall 0.22896228628578505, test_topK_precision 0.20442622950819675\n",
      "epoch-batch_idx: 21-77: train_loss: 0.06545618176460266   test_topK_recall 0.22970723061697768, test_topK_precision 0.20450819672131149\n"
     ]
    }
   ],
   "source": [
    "light_loss, light_bpr, light_reg, light_recall, light_precision = train_and_eval(lightgcn, optimizer, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCOJY4XST38b"
   },
   "outputs": [],
   "source": [
    "epoch_list = [(i+1) for i in range(EPOCHS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5P2Zf6yT4Uu"
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_list, light_loss, label='Total Training Loss')\n",
    "plt.plot(epoch_list, light_bpr, label='BPR Training Loss')\n",
    "plt.plot(epoch_list, light_reg, label='Reg Training Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1Quk5mahJ1n"
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_list, light_recall, label='Recall')\n",
    "plt.plot(epoch_list, light_precision, label='Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yl6OL1UPzqju"
   },
   "source": [
    "### Train and eval NGCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwO_h55Jzj4J"
   },
   "outputs": [],
   "source": [
    "ngcf = RecSysGNN(\n",
    "    latent_dim=latent_dim, \n",
    "    num_layers=n_layers,\n",
    "    num_users=n_users,\n",
    "    num_items=n_items,\n",
    "    model='NGCF'\n",
    ")\n",
    "ngcf.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(ngcf.parameters(), lr=LR)\n",
    "print(\"Size of Learnable Embedding : \", [x.shape for x in list(ngcf.parameters())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUtZEhAEzjrA"
   },
   "outputs": [],
   "source": [
    "ngcf_loss, ngcf_bpr, ngcf_reg, ngcf_recall, ngcf_precision = train_and_eval(ngcf, optimizer, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENivAkwTzjcP"
   },
   "outputs": [],
   "source": [
    "epoch_list = [(i+1) for i in range(EPOCHS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MxNmjo-T4mg"
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_list, ngcf_loss, label='Total Training Loss')\n",
    "plt.plot(epoch_list, ngcf_bpr, label='BPR Training Loss')\n",
    "plt.plot(epoch_list, ngcf_reg, label='Reg Training Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnVhVglYpd7G"
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_list, ngcf_recall, label='Recall')\n",
    "plt.plot(epoch_list, ngcf_precision, label='Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metrics')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myUn4-Sr26KT"
   },
   "source": [
    "### Compare model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MY7QibZ6NNJ"
   },
   "outputs": [],
   "source": [
    "max(light_precision), max(light_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbxiFcIe3BAu"
   },
   "outputs": [],
   "source": [
    "max(ngcf_precision), max(ngcf_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RNPSfjWjV25"
   },
   "source": [
    "## Paper References\n",
    "\n",
    "1. Harper, F. Maxwell, and Konstan, Joseph A. “The MovieLensDatasets: History and Context.” ACM Transactions on Interactive Intelligence Systems (TiiS) 5, 4. 2015.\n",
    "2. He, Xiangnan, et al. “LightGCN: Simplifying and powering graph convolution network for recommendation.” Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2020.\n",
    "3. Wang, Xiang, et al. “Neural graph collaborative filtering.” Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. 2019.\n",
    "\n",
    "## Code References\n",
    "\n",
    "We thank the authors of the following codebases and notebooks, from which parts of this tutorial were inspired or adapted.\n",
    "\n",
    "- https://www.kaggle.com/dipanjandas96/lightgcn-pytorch-from-scratch\n",
    "\n",
    "- https://github.com/gusye1234/LightGCN-PyTorch\n",
    "\n",
    "- https://github.com/SytzeAndr/NGCF_RP32/blob/hand-in/NGCF.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
