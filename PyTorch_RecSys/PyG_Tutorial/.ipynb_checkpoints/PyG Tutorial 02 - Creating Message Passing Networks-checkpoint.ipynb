{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ebdef35",
   "metadata": {},
   "source": [
    "### Creating Message Passing Networks\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
    "    \n",
    "这个感觉还挺重要的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98763b64",
   "metadata": {},
   "source": [
    "\"For bipartite graphs with two independent sets of nodes and indices, and each set holding its own information, this split can be marked by passing the information as a tuple, e.g. x=(x_N, x_M)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94acc3f5",
   "metadata": {},
   "source": [
    "__Tip__:（发现这里 两个 dollar sign 是可以写 TeX command 单独一行, 一个 dollar sign 是blend into text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc64ac",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----------\n",
    "General Message Passing 格式如下\n",
    "\n",
    "$$ \\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\right), $$\n",
    "\n",
    "$\\gamma^{(k)} $: 是所谓的  differentiable functions such as MLPs (Multi Layer Perceptrons), 说直接点，就是 $\\sigma$ (sigmoid, ReLu..) 这种 activation function 然后里面套一个 linear function 比如 __W__ * __x__ + __B__. 文档里说 __MessagePassing.update(aggr_out, ...)__ 就是做这个的\n",
    "\n",
    "$\\phi^{(k)}$: 也是所谓的  differentiable functions such as MLPs (Multi Layer Perceptrons)。 文档里说 __MessagePassing.message(..)__ 就是做这个的\n",
    "\n",
    "$\\square_{j \\in \\mathcal{N}(i)} $ 是aggregation func, 即differentiable, permutation invariant function, e.g., sum, mean or max\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4f907",
   "metadata": {},
   "source": [
    "-----------\n",
    "对照 GCN Layer\n",
    "$$ \\mathbf{x}_i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot \\left( \\mathbf{W}^{\\top} \\cdot \\mathbf{x}_j^{(k-1)} \\right) + \\mathbf{b}, $$\n",
    "\n",
    "- 他这边好像只有 $\\phi^{(k)}$ 这个了， 但是没有 activation function ??\n",
    "- 这个看似少了 $\\mathbf{x}_i^{(k-1)}$, 但是其实是因为假设了有 self-loop, 你看下面代码也有特意加 self-loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6d51d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Parameter\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    # Q: in channel , out channel 啥意思？\n",
    "    # A: 哦，这里是给 linear function 用的，其实就是 input output 你想要的 dimensions\n",
    "    # 比如我如果 x 每个 node 只有一个 feature， 然后我需要 linear 过完只有一个 output， 那么就是 1 x 1 就行\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.lin = Linear(in_channels, out_channels, bias=False)\n",
    "        # 这边 bias term 直接用 out channel 数.. 神奇,不过反正是 init\n",
    "        self.bias = Parameter(torch.Tensor(out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # not sure what this is for??    \n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # N here is node number?\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        print(f\"edge_index without self loop {edge_index}\")\n",
    "        \n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        print(f\"edge_index with self loop {edge_index}\")\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        # 为啥？？哦，因为 这个是 Linear(in_channels, out_channels, bias=False) \n",
    "        x = self.lin(x)\n",
    "        \n",
    "        print(f\"x after linear layer {x}\")\n",
    "\n",
    "        # Step 3: Compute normalization.\n",
    "        # 这是啥？edge_index 还能拆？不是 from -> to 两个 list?\n",
    "        # 所以就是拿了 edge_index 的两个 list, 本质就是 from_list 和 to_list\n",
    "        # 哦，我知道了，这边这个图示解释的非常清楚： https://matteding.github.io/2019/04/25/sparse-matrices/\n",
    "        # 当然上面链接是假设 edge 数值不为 1, 我们这里假设为1， 那么 from_list 和 to_list 本质上就是 sparse matrix\n",
    "        # 里面如果用 adjescent matrix 来表示的 row 和 col, \n",
    "        #             比如 [[0, 1, 1, 2],\n",
    "        #                  [1, 0, 2, 1]]\n",
    "        # 这里 row[0] = 0, col[0] = 1 也就是在 adjescent matrix 的 第 0 row， 和 第 1 col 的位置，有一个 edge\n",
    "         \n",
    "        row, col = edge_index\n",
    "        \n",
    "        print(f\"row {row}\")\n",
    "        print(f\"col {col}\")\n",
    "        \n",
    "        # Q: 这个不是很懂，但是大概知道是算 neighbor cnt 的\n",
    "        # col, 和 node count 穿进去就能算 所有node 的 degree ?\n",
    "        # A: 哦，是可以，比如 row tensor([0, 1, 1, 2, 0, 1, 2]), 那么我直接看比如 0 出现几次， 1 出现，几次, 2 \n",
    "        # 出现几次，然后总的 node 有多少个，就可以算出 每个 node 的 degree\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        \n",
    "        print(f\"deg {deg}\")\n",
    "        \n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        \n",
    "        print(f\"deg_inv_sqrt {deg_inv_sqrt}\")\n",
    "        \n",
    "        # 这啥？为了防止1 ／ sqrt root得到正无穷？ 我除法在哪没找到呀? 哦.pow(-0.5), 这边 0.5 是 sqrt 然后 负号就是 inverse\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        \n",
    "        print(f\"deg_inv_sqrt {deg_inv_sqrt}\")\n",
    "        \n",
    "        # 这个就是上面的 normalization term, 所以 row, col 这里是表示啥？(上面 deg 解释了)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        \n",
    "        print(f\"norm {norm}\")\n",
    "        \n",
    "        \n",
    "\n",
    "        # Step 4-5: Start propagating messages.\n",
    "        out = self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "        print(f\"out without bias term: {out}\")\n",
    "        \n",
    "        # Step 6: Apply a final bias vector.\n",
    "        out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]  E 这里指的是 Edge 数, out_channel 其实就是 每个 node 的 feature 数？\n",
    "        # x_j 是啥？？\n",
    "        # “Here, x_j denotes a lifted tensor, which contains the source node features of each edge, \n",
    "        # i.e., the neighbors of each node. Node features can be automatically lifted by appending _i or _j \n",
    "        # to the variable name. ”\n",
    "        # 还是不很清楚这里砸弄的?? 这里的 x_j 为什么 dimension 跟 edge_index 的 dim 一样？\n",
    "        # 哦，因为是 the source node features of each edge, \n",
    "        # 比如 row tensor([0, 1, 1, 2, 0, 1, 2])\n",
    "        #  x after linear layer tensor([[ 0.0453], [ 0.0000], [-0.0453]])\n",
    "        # 那么 x_j 就是 tensor([[ 0.0453], [ 0.0000], [ 0.0000], [-0.0453], [ 0.0453], [ 0.0000], [-0.0453]])\n",
    "        # 你看是一一对应的\n",
    "\n",
    "        print(f\"x_j {x_j}\")\n",
    "        \n",
    "        # Step 4: Normalize node features.\n",
    "        print(f\"norm.view(-1, 1) {norm.view(-1, 1)}\")\n",
    "        \n",
    "        # 这里 reshape 一下？x_j 是啥？怎么传的？ propagate 跟 message 啥关系？我感觉的是 propagate 会call message\n",
    "        # 文档说 message 类似 theta(..) 但是我这里怎么就之乘一下？\n",
    "        # “We then call propagate(), which internally calls message(), aggregate() and update()”\n",
    "        # agg 在 init 的时候就 define 了 super().__init__(aggr='add')\n",
    "        \n",
    "        # 既然 x_j 是跟 edge_index 的 dim1 一个 dimension， 那么我觉得 magic 应该是在agg() update() 里面\n",
    "        # 因为 message() 这里就是 \"source node features of each edge\" 乘上 normalization term, \n",
    "        # 然后 agg() 反正知道每个 node 的 neighbor 是啥，就 aggregate 一下，\n",
    "        # 但是这里只看公式，你还真多不知道 message 得这么写..?\n",
    "        # 哦，大概明白了!! 这里之所以用 source node features of each edge\" ， 是因为我们这里的 message flow \n",
    "        # 是 source-> target node\n",
    "        # 所以这里传 x_j 估计是为了 internal 好算，因为我比如需要知道 node2 的 neighbor， 我直接在 x_j 里面 和 row list\n",
    "        # 里面，找node2 出现的地方，就可以找到 node2 点neighbor 的 message 了!\n",
    "        return norm.view(-1, 1) * x_j\n",
    "    \n",
    "    \n",
    "    # Question: 这里更奇怪的是，我看有的 implementation 都直接不写  message??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b4fc2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index without self loop tensor([[0, 1, 1, 2],\n",
      "        [1, 0, 2, 1]])\n",
      "edge_index with self loop tensor([[0, 1, 1, 2, 0, 1, 2],\n",
      "        [1, 0, 2, 1, 0, 1, 2]])\n",
      "x after linear layer tensor([[ 0.0453],\n",
      "        [ 0.0000],\n",
      "        [-0.0453]])\n",
      "row tensor([0, 1, 1, 2, 0, 1, 2])\n",
      "col tensor([1, 0, 2, 1, 0, 1, 2])\n",
      "deg tensor([2., 3., 2.])\n",
      "deg_inv_sqrt tensor([0.7071, 0.5774, 0.7071])\n",
      "deg_inv_sqrt tensor([0.7071, 0.5774, 0.7071])\n",
      "norm tensor([0.4082, 0.4082, 0.4082, 0.4082, 0.5000, 0.3333, 0.5000])\n",
      "x_j tensor([[ 0.0453],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-0.0453],\n",
      "        [ 0.0453],\n",
      "        [ 0.0000],\n",
      "        [-0.0453]])\n",
      "norm.view(-1, 1) tensor([[0.4082],\n",
      "        [0.4082],\n",
      "        [0.4082],\n",
      "        [0.4082],\n",
      "        [0.5000],\n",
      "        [0.3333],\n",
      "        [0.5000]])\n",
      "out without bias term: tensor([[ 0.0226],\n",
      "        [ 0.0000],\n",
      "        [-0.0226]])\n"
     ]
    }
   ],
   "source": [
    "# 借用 tutorial 1 的 data 手动跑一下 forward\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "\n",
    "# create a 3x1 tensor matrix\n",
    "# this is edge feature??\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv = GCNConv(1, 1)\n",
    "    x = conv(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b02169a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1353],\n",
      "        [ 0.0000],\n",
      "        [ 0.1353]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71b08d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来可以跳过了，这里该挖的挖的差不多了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fbf975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
