{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning Tutorial - Lightweight PyTorch Wrapper For ML Researchers\n",
    "- https://www.youtube.com/watch?v=Hgg8Xy6IRig\n",
    "- from: PyTorch Course by Python Engineer Channel\n",
    "\n",
    "\n",
    "Let's see how much easier PyTorch is and how easy it is to write a training loop \n",
    "and maybe tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "# 2.1.2\n",
    "print(pl.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below are modified from: \n",
    "- https://github.com/patrickloeber/pytorchTutorial/blob/master/13_feedforward.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "input_size = 784  # 28x28\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "# trainer = Trainer(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.MNIST(root=os.getcwd(),  #  get path of the current working directory\n",
    "                                       download=True, \n",
    "                                       train=True, \n",
    "                                       transform=transforms.ToTensor())\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root=os.getcwd(), \n",
    "                                      download=True, \n",
    "                                      train=False, \n",
    "                                      transform=transforms.ToTensor())\n",
    "\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_set) * 0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, valid_set = torch.utils.data.random_split(train_set, \n",
    "                                         [train_set_size, valid_set_size], \n",
    "                                         generator=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "\n",
    "# class NeuralNet(nn.Module):\n",
    "class LitNeuralNet(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LitNeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)  \n",
    "        self.validation_outputs = []\n",
    "        self.test_outputs = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n",
    "\n",
    "\n",
    "    # also need to add trainig setup and optimizer to the class\n",
    "    # https://lightning.ai/docs/pytorch/stable/starter/introduction.html\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        '''\n",
    "            This is the training loop that we simplified that was looking like this:\n",
    "            https://github.com/patrickloeber/pytorchTutorial/blob/master/13_feedforward.py#L68-L86\n",
    "        '''\n",
    "        images, y = batch # unpack batch\n",
    "\n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        # reshape in such so each image pixel is a numeric feature and 100 is the batch_size\n",
    "        images = images.reshape(-1, 28  * 28 )\n",
    "\n",
    "        y_pred = self(images) # pred_y\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        '''\n",
    "        compare to the not using the pytorch lighgting\n",
    "            - the only things on left out here is not having to write .to(device) and .zero_grad() , .backward() and .step()\n",
    "            - so essentially it's nice to see Lightning allows me to skip a lot of the manual looping and optimizer/loss management\n",
    "\n",
    "            # origin shape: [100, 1, 28, 28]\n",
    "            # resized: [100, 784]\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        '''\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self): \n",
    "        \n",
    "        # dataset = torchvision.datasets.MNIST(os.getcwd(),  #  get path of the current working directory\n",
    "        #                                      train=True, \n",
    "        #                                      download=True, \n",
    "        #                                      transform = transforms.ToTensor())  \n",
    "          \n",
    "        loader =  torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                              batch_size=batch_size, \n",
    "                                              num_workers=4, # use multi-core to load data\n",
    "                                              shuffle=False)\n",
    "\n",
    "        return loader\n",
    "    \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        '''\n",
    "            This is the training loop that we simplified that was looking like this:\n",
    "            https://github.com/patrickloeber/pytorchTutorial/blob/master/13_feedforward.py#L68-L86\n",
    "        '''\n",
    "        images, y = batch # unpack batch\n",
    "\n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        # reshape in such so each image pixel is a numeric feature and 100 is the batch_size\n",
    "        images = images.reshape(-1, 28  * 28 )\n",
    "\n",
    "        y_pred = self(images) # pred_y\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        tensorboard_logs = {'val_loss': loss}\n",
    "\n",
    "        # self.log('val_loss', loss)\n",
    "\n",
    "        self.validation_outputs.append(loss)\n",
    "\n",
    "        return {'val_loss': loss, 'log': tensorboard_logs}  \n",
    "\n",
    "    def val_dataloader(self): \n",
    "        '''\n",
    "            官方文档好像有点不一样？用的同一个 validation DataLoader 就是不同的 input\n",
    "                https://lightning.ai/docs/pytorch/stable/common/evaluation_basic.html\n",
    "\n",
    "            train_loader = DataLoader(train_set)\n",
    "            valid_loader = DataLoader(valid_set)\n",
    "\n",
    "            # train with both splits\n",
    "            trainer = L.Trainer()\n",
    "            trainer.fit(model, train_loader, valid_loader)    \n",
    "\n",
    "            视频里偷懒了，直接把  test set 当作 validation set 了.. 我改正确吧.\n",
    "        '''\n",
    "\n",
    "        loader =  torch.utils.data.DataLoader(dataset=valid_set, \n",
    "                                              batch_size=batch_size, \n",
    "                                              num_workers=4, # use multi-core to load data\n",
    "                                              shuffle=False)\n",
    "\n",
    "        return loader      \n",
    "    \n",
    "\n",
    "    # NotImplementedError: Support for `validation_epoch_end` has been removed in v2.0.0. `LitNeuralNet` implements this method. \n",
    "    # You can use the `on_validation_epoch_end` hook instead. To access outputs, save them in-memory as instance attributes. You can find migration examples in https://github.com/Lightning-AI/lightning/pull/16520.\n",
    "    def on_validation_epoch_end(self):\n",
    "        '''\n",
    "            what am i expect ot see with this?\n",
    "             好像是 training 的 progress bar 如果没有这个，就不会显示 loss, 变成看不到..? \n",
    "        '''\n",
    "        # this is because we \"return {'val_loss': loss, 'log': tensorboard_logs}\"  \n",
    "        # in the validation_step  \n",
    "        # print(self.validation_outputs)\n",
    "\n",
    "        avg_loss = torch.stack(self.validation_outputs).mean()\n",
    "\n",
    "        self.log('val_loss', avg_loss, on_epoch=True) # this will alow trainer.fit() progress bar showing loss?? no it doesn'..\n",
    "\n",
    "        print(f' val_loss: {avg_loss}') # have to manually printed out... so lame...\n",
    "\n",
    "        self.validation_outputs = [] # reset to empty for the next epoch..\n",
    "\n",
    "        return {'val_loss': avg_loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):        \n",
    "        images, y = batch # unpack batch\n",
    "\n",
    "        # origin shape: [100, 1, 28, 28]\n",
    "        # resized: [100, 784]\n",
    "        # reshape in such so each image pixel is a numeric feature and 100 is the batch_size\n",
    "        images = images.reshape(-1, 28  * 28 )\n",
    "\n",
    "        # print(f'images.max(): {images.max()}')\n",
    "\n",
    "        y_pred = self(images) # pred_y\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        tensorboard_logs = {'test_loss': loss}\n",
    "\n",
    "        self.test_outputs.append(loss)\n",
    "\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "        return {'test_loss': loss, 'log': tensorboard_logs}  \n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "    \n",
    "        avg_loss = torch.stack(self.test_outputs).mean()\n",
    "\n",
    "        self.log('test_loss_agg', avg_loss, on_epoch=True) # this will alow trainer.fit() progress bar showing loss?? no it doesn'..\n",
    "\n",
    "        print(f' test_loss_agg: {avg_loss}') # have to manually printed out... so lame...\n",
    "\n",
    "        self.test_outputs = [] # reset to empty for the next epoch..\n",
    "\n",
    "        return {'test_loss_agg': avg_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:230] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# trainer is the training loop\n",
    "\n",
    "# trainer = Trainer(fast_dev_run=True) # fast_dev_run only run one batch so you can test if your model works!!\n",
    "trainer = Trainer(max_epochs=num_epochs, \n",
    "                  fast_dev_run=False\n",
    "                  )\n",
    "\n",
    "model = LitNeuralNet(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | l1   | Linear | 392 K \n",
      "1 | relu | ReLU   | 0     \n",
      "2 | l2   | Linear | 5.0 K \n",
      "--------------------------------\n",
      "397 K     Trainable params\n",
      "0         Non-trainable params\n",
      "397 K     Total params\n",
      "1.590     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 67.76it/s] val_loss: 2.290222644805908\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 480/480 [00:21<00:00, 22.53it/s, v_num=21] val_loss: 0.18457770347595215\n",
      "Epoch 1: 100%|██████████| 480/480 [00:39<00:00, 12.05it/s, v_num=21] val_loss: 0.12921838462352753\n",
      "Epoch 1: 100%|██████████| 480/480 [00:53<00:00,  8.95it/s, v_num=21]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 480/480 [00:53<00:00,  8.93it/s, v_num=21]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Intead of doing a double for loop to iterate through epoches and batches like the old way\n",
    "        n_total_steps = len(train_loader)\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (images, labels) in enumerate(train_loader):  \n",
    "                ...\n",
    "\n",
    "    you can simply do the following\n",
    "'''\n",
    "\n",
    "train_loader = DataLoader(train_set,\n",
    "                          batch_size=batch_size, \n",
    "                          num_workers=4, # use multi-core to load data\n",
    "                          shuffle=False)\n",
    "\n",
    "# must set shuffle=False otherwise will get the following error:\n",
    "# PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.category=PossibleUserWarning,\n",
    "# Question: Why? Answer: according to chatGPT, it's to ensure Reproducibility to ensures consistency in evaluation metrics across different runs and models.\n",
    "valid_loader = DataLoader(valid_set,\n",
    "                          batch_size=batch_size, \n",
    "                          num_workers=4, # use multi-core to load data\n",
    "                          shuffle=False) \n",
    " \n",
    "\n",
    "# TODO: need to understand some of these parameters..!\n",
    "trainer.fit(model=model,\n",
    "            # gpus=1, # if you have GPU haha\n",
    "            # deterministic= True, # make it more reproduceable\n",
    "            # auto_lr_find=True,\n",
    "            # gradient_clip_val, # some value between 0 and 1??\n",
    "            train_dataloaders=train_loader, # this overrides the val_dataloader you defined I assume??\n",
    "            val_dataloaders=valid_loader\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform on test set!!\n",
    "# todo: need to look into how to create a custom Dataset in pytorch\n",
    "test_loader = DataLoader(dataset=test_set,\n",
    "                          batch_size=batch_size, \n",
    "                          num_workers=4, # use multi-core to load data\n",
    "                          shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch_py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 100/100 [00:01<00:00, 63.48it/s] test_loss_agg: 0.10936376452445984\n",
      "Testing DataLoader 0: 100%|██████████| 100/100 [00:01<00:00, 63.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10936377197504044    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_loss_agg       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10936376452445984    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10936377197504044   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_loss_agg      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10936376452445984   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.10936377197504044, 'test_loss_agg': 0.10936376452445984}]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wait, but where is the output? \n",
    "# oh nice!! it even printed out a Test Metric table ~, maybe it's because self.log()?\n",
    "trainer.test(model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: lightning_logs/version19/checkpoints/epoch=1-step=960.ckpt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls lightning_logs/version19/checkpoints/epoch=1-step=960.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# perform single inference\n",
    "\n",
    "# issue: __init__() missing 3 required positional arguments: 'input_size', 'hidden_size', and 'num_classes'??\n",
    "# solved: wow.. this is so dum,... I have to input all these params even when loading the model? \n",
    "model = LitNeuralNet.load_from_checkpoint(checkpoint_path=\"lightning_logs/version_8/checkpoints/epoch=1-step=960.ckpt\",\n",
    "                                        input_size=input_size,\n",
    "                                        hidden_size=hidden_size,\n",
    "                                        num_classes=num_classes)\n",
    "model.eval()\n",
    "x = torch.rand(1, 28 * 28)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat = model(x)\n",
    "\n",
    "print(torch.argmax(y_hat, dim=1).item()) # use item to unwrap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "Question: __How to use pytorh lightning with tensorboard__?\n",
    "\n",
    "Install dependencies\n",
    "- pip install lightning[app]\n",
    "- pip install tensorboard\n",
    "\n",
    "Run the following Command\n",
    "- tensorboard --logdir=lightning_logs\n",
    "\n",
    "Common issues when tensorboard not working:\n",
    "- restart 一下 notebook kernal 就行， 然后确认一下，下面这个 event path 有在你的 log 里面 `events.out.tfevents.1701028601.Qis-MacBook-Pro-2.local.14682.0`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "*Question*: why is my v_num= empty? Epoch 0: 100%|██████████| 2/2 [00:00<00:00,  4.29it/s, loss=2.32, v_num=]\n",
    "- *Answer*: 好像 fast_dev_run=True 然后用 lightning 2.x 就可以有了\n",
    "    - Epoch 0: 100%|██████████| 480/480 [00:19<00:00, 25.11it/s, v_num=2]\n",
    "\n",
    "*Question*: Difference between Lightning and Pytorch-Lightning?\n",
    "- *Answer*: 好像一个是 library 一个是 command line tool    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 官方文档好好过一遍: https://lightning.ai/docs/pytorch/stable/starter/introduction.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Level Summary\n",
    "- 大体说，就是让你用一些类似于 sklearn 的 api ， 比如 .fit(), .predict() 这些，来写pytorch model\n",
    "\n",
    "### Other thoughts\n",
    "- 官方文档改的好烂..\n",
    "- 然后有一些好的 param 和 logging 好像也没有了.. 非常扯，比如我想要的 val_loss 和 test_loss 都没有自动带..\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
